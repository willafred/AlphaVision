{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# Evaluation (Metrics & DM-Test)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from itertools import islice\n",
    "from math import lgamma, fabs, isnan, nan, exp, log, log1p, sqrt\n",
    "from typing import Sequence, Callable, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_ff_factors_100325.csv\")\n",
    "df = df.drop(columns=[\"Unnamed: 0\",\"crsp_portno\"])\n",
    "df = df.sort_values(by='date')\n",
    "df\n",
    "\n",
    "df_tech = pd.read_csv(\"df_ff_factors_techfunds.csv\")\n",
    "df_tech = df_tech.drop(columns=[\"Unnamed: 0\",\"crsp_portno\"])\n",
    "df_tech = df_tech.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunk below describes the various functions involved in the training of the models, which are described in greater depth below:\n",
    "\n",
    "(1) Generation of lagged dataframe i.e. lagging the characteristics\n",
    "\n",
    "(2) Generation of stepped dataframe i.e. lagged characteristics being together with the 1-month ahead forecast\n",
    "\n",
    "(3) Demeaning function for fund-level characteristics, which is important to ensure accuracy in the overall forecasting of annualised alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Lagged Dataset\n",
    "def create_lagged_dataset(dataset, lag, target_var, id):\n",
    "    lagged_dataset = dataset.copy()\n",
    "    columns_list = list(lagged_dataset.columns)\n",
    "    data_join = {}\n",
    "    for column in columns_list:\n",
    "        if column == target_var:\n",
    "            data_join[column] = lagged_dataset[column]\n",
    "        for n in range(1,lag+1):\n",
    "            data_join[F'{column}_L{n}'] = lagged_dataset.groupby(id)[column].shift(n)\n",
    "    lagged_dataset = pd.concat(data_join.values(), axis=1, ignore_index = True)\n",
    "    lagged_dataset.columns = data_join.keys()\n",
    "    return lagged_dataset.dropna()\n",
    "\n",
    "# Generate Stepped Dataset for Training\n",
    "## Steps is the number of months ahead that we are forecasting, e.g. step=2 is 2 months ahead.\n",
    "## Note step=1 results in no change to dataset, i.e. use generated lagged variables to forecast current. \n",
    "def create_stepped_dataset(dataset, step, target_var, id):\n",
    "    \n",
    "    shifted_dataset = dataset.copy()\n",
    "    shifted_dataset['shifted_target'] = shifted_dataset.groupby(id)[target_var].shift(-step + 1)\n",
    "    \n",
    "    # Drop rows where the shifted target is NaN (these occur due to the shift operation)\n",
    "    shifted_dataset = shifted_dataset.dropna(subset=['shifted_target'])\n",
    "    \n",
    "    # Separate the features (X) and the target (y)\n",
    "    X = shifted_dataset.drop(columns=[target_var, 'shifted_target'])\n",
    "    y = shifted_dataset[['shifted_target']]\n",
    "    y = y.rename(columns={'shifted_target':target_var})\n",
    "    return X, y\n",
    "\n",
    "def demeaning_fund_char(dataset, id, characteristic):\n",
    "    for col in characteristic:\n",
    "        dataset[f'demeaned_{col}'] = dataset[col] - dataset.groupby(id)[col].transform('mean')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Temp\\ipykernel_20108\\1523674970.py:4: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['normalised_flow'] = df['normalised_flow'].fillna(method='ffill')\n",
      "C:\\Users\\wjlwi\\AppData\\Local\\Temp\\ipykernel_20108\\1523674970.py:10: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_tech['normalised_flow'] = df_tech['normalised_flow'].fillna(method='ffill')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crsp_fundno</th>\n",
       "      <th>date</th>\n",
       "      <th>mth_return</th>\n",
       "      <th>exp_ratio</th>\n",
       "      <th>turn_ratio</th>\n",
       "      <th>normalised_flow</th>\n",
       "      <th>gdp_to_debt_ratio</th>\n",
       "      <th>gdp_growth_rate</th>\n",
       "      <th>unm_rate</th>\n",
       "      <th>infl_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_sharpe</th>\n",
       "      <th>mkt_return</th>\n",
       "      <th>rolling_alpha_3f</th>\n",
       "      <th>rolling_alpha_4f</th>\n",
       "      <th>rolling_alpha_5f</th>\n",
       "      <th>shortrun_momentum</th>\n",
       "      <th>demeaned_exp_ratio</th>\n",
       "      <th>demeaned_turn_ratio</th>\n",
       "      <th>demeaned_normalised_flow</th>\n",
       "      <th>demeaned_shortrun_momentum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43631</th>\n",
       "      <td>32553.0</td>\n",
       "      <td>1993-08-31</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.009529</td>\n",
       "      <td>64.101</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.90</td>\n",
       "      <td>...</td>\n",
       "      <td>3.248937</td>\n",
       "      <td>0.056070</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.464384</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.020627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43632</th>\n",
       "      <td>32553.0</td>\n",
       "      <td>1993-09-30</td>\n",
       "      <td>-0.004082</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>64.101</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.90</td>\n",
       "      <td>...</td>\n",
       "      <td>2.216452</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.464384</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>-0.001352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43633</th>\n",
       "      <td>32553.0</td>\n",
       "      <td>1993-10-29</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>64.669</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.90</td>\n",
       "      <td>...</td>\n",
       "      <td>3.058133</td>\n",
       "      <td>0.021609</td>\n",
       "      <td>0.040962</td>\n",
       "      <td>0.040962</td>\n",
       "      <td>0.040962</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>-0.464384</td>\n",
       "      <td>0.022620</td>\n",
       "      <td>0.008144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13747</th>\n",
       "      <td>12051.0</td>\n",
       "      <td>1993-10-29</td>\n",
       "      <td>0.034712</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.070030</td>\n",
       "      <td>64.669</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.90</td>\n",
       "      <td>...</td>\n",
       "      <td>5.744340</td>\n",
       "      <td>0.021609</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.007662</td>\n",
       "      <td>-0.699757</td>\n",
       "      <td>0.066601</td>\n",
       "      <td>0.016976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748</th>\n",
       "      <td>12051.0</td>\n",
       "      <td>1993-11-30</td>\n",
       "      <td>-0.075974</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>64.669</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.90</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.536935</td>\n",
       "      <td>-0.010806</td>\n",
       "      <td>-0.075999</td>\n",
       "      <td>-0.075999</td>\n",
       "      <td>-0.075999</td>\n",
       "      <td>-0.026033</td>\n",
       "      <td>0.007662</td>\n",
       "      <td>-0.699757</td>\n",
       "      <td>-0.003170</td>\n",
       "      <td>-0.022160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>4610.0</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>0.078565</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.650182</td>\n",
       "      <td>120.040</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.35</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619481</td>\n",
       "      <td>0.055058</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>-0.025985</td>\n",
       "      <td>-0.001211</td>\n",
       "      <td>-1.434538</td>\n",
       "      <td>-0.575797</td>\n",
       "      <td>-0.025378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>4330.0</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>0.028831</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.022800</td>\n",
       "      <td>120.731</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691406</td>\n",
       "      <td>0.055058</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>-0.024241</td>\n",
       "      <td>-0.000815</td>\n",
       "      <td>-0.039661</td>\n",
       "      <td>-0.033919</td>\n",
       "      <td>-0.013655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>4333.0</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.402964</td>\n",
       "      <td>120.731</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691738</td>\n",
       "      <td>0.055058</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>-0.024228</td>\n",
       "      <td>-0.000737</td>\n",
       "      <td>-0.039661</td>\n",
       "      <td>-0.817815</td>\n",
       "      <td>-0.013661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>4327.0</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>0.028616</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.032107</td>\n",
       "      <td>120.731</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676394</td>\n",
       "      <td>0.055058</td>\n",
       "      <td>0.004506</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>0.010697</td>\n",
       "      <td>-0.024451</td>\n",
       "      <td>-0.000815</td>\n",
       "      <td>-0.039661</td>\n",
       "      <td>-0.042810</td>\n",
       "      <td>-0.013657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>4329.0</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>0.027868</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-1.025886</td>\n",
       "      <td>120.731</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631615</td>\n",
       "      <td>0.055058</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>-0.025131</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>-0.039661</td>\n",
       "      <td>-1.419450</td>\n",
       "      <td>-0.013705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54718 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       crsp_fundno        date  mth_return  exp_ratio  turn_ratio  \\\n",
       "43631      32553.0  1993-08-31    0.068935     0.0162        0.15   \n",
       "43632      32553.0  1993-09-30   -0.004082     0.0162        0.15   \n",
       "43633      32553.0  1993-10-29    0.040984     0.0162        0.15   \n",
       "13747      12051.0  1993-10-29    0.034712     0.0188        0.77   \n",
       "13748      12051.0  1993-11-30   -0.075974     0.0188        0.77   \n",
       "...            ...         ...         ...        ...         ...   \n",
       "3031        4610.0  2024-06-28    0.078565     0.0217        0.23   \n",
       "1960        4330.0  2024-07-31    0.028831     0.0077        0.31   \n",
       "2113        4333.0  2024-07-31    0.028826     0.0076        0.31   \n",
       "1842        4327.0  2024-07-31    0.028616     0.0102        0.31   \n",
       "1901        4329.0  2024-07-31    0.027868     0.0178        0.31   \n",
       "\n",
       "       normalised_flow  gdp_to_debt_ratio  gdp_growth_rate  unm_rate  \\\n",
       "43631         0.009529             64.101           3.5225       6.8   \n",
       "43632         0.014881             64.101           3.5225       6.7   \n",
       "43633         0.030641             64.669           3.5225       6.8   \n",
       "13747         0.070030             64.669           3.5225       6.8   \n",
       "13748         0.000259             64.669           3.5225       6.6   \n",
       "...                ...                ...              ...       ...   \n",
       "3031         -0.650182            120.040           2.5427       4.1   \n",
       "1960         -0.022800            120.731           2.5427       4.2   \n",
       "2113         -0.402964            120.731           2.5427       4.2   \n",
       "1842         -0.032107            120.731           2.5427       4.2   \n",
       "1901         -1.025886            120.731           2.5427       4.2   \n",
       "\n",
       "       infl_rate  ...  rolling_sharpe  mkt_return  rolling_alpha_3f  \\\n",
       "43631       2.90  ...        3.248937    0.056070          0.068910   \n",
       "43632       2.90  ...        2.216452    0.027009         -0.004108   \n",
       "43633       2.90  ...        3.058133    0.021609          0.040962   \n",
       "13747       2.90  ...        5.744340    0.021609          0.034690   \n",
       "13748       2.90  ...       -0.536935   -0.010806         -0.075999   \n",
       "...          ...  ...             ...         ...               ...   \n",
       "3031        3.35  ...        1.619481    0.055058          0.001428   \n",
       "1960        3.35  ...        0.691406    0.055058          0.004765   \n",
       "2113        3.35  ...        0.691738    0.055058          0.004733   \n",
       "1842        3.35  ...        0.676394    0.055058          0.004506   \n",
       "1901        3.35  ...        0.631615    0.055058          0.003868   \n",
       "\n",
       "       rolling_alpha_4f  rolling_alpha_5f  shortrun_momentum  \\\n",
       "43631          0.068910          0.068910           0.012866   \n",
       "43632         -0.004108         -0.004108          -0.009113   \n",
       "43633          0.040962          0.040962           0.000383   \n",
       "13747          0.034690          0.034690           0.013103   \n",
       "13748         -0.075999         -0.075999          -0.026033   \n",
       "...                 ...               ...                ...   \n",
       "3031           0.002329          0.003201          -0.025985   \n",
       "1960           0.005643          0.010978          -0.024241   \n",
       "2113           0.005601          0.010953          -0.024228   \n",
       "1842           0.005384          0.010697          -0.024451   \n",
       "1901           0.004746          0.010018          -0.025131   \n",
       "\n",
       "       demeaned_exp_ratio  demeaned_turn_ratio  demeaned_normalised_flow  \\\n",
       "43631            0.000025            -0.464384                  0.001507   \n",
       "43632            0.000025            -0.464384                  0.006859   \n",
       "43633            0.000025            -0.464384                  0.022620   \n",
       "13747            0.007662            -0.699757                  0.066601   \n",
       "13748            0.007662            -0.699757                 -0.003170   \n",
       "...                   ...                  ...                       ...   \n",
       "3031            -0.001211            -1.434538                 -0.575797   \n",
       "1960            -0.000815            -0.039661                 -0.033919   \n",
       "2113            -0.000737            -0.039661                 -0.817815   \n",
       "1842            -0.000815            -0.039661                 -0.042810   \n",
       "1901            -0.000754            -0.039661                 -1.419450   \n",
       "\n",
       "       demeaned_shortrun_momentum  \n",
       "43631                    0.020627  \n",
       "43632                   -0.001352  \n",
       "43633                    0.008144  \n",
       "13747                    0.016976  \n",
       "13748                   -0.022160  \n",
       "...                           ...  \n",
       "3031                    -0.025378  \n",
       "1960                    -0.013655  \n",
       "2113                    -0.013661  \n",
       "1842                    -0.013657  \n",
       "1901                    -0.013705  \n",
       "\n",
       "[54718 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'dataset' is your DataFrame and 'normalized_flow' is the column with NaN values\n",
    "df['normalised_flow'] = df['normalised_flow'].fillna(method='ffill')\n",
    "char_to_demean = [\"exp_ratio\", \"turn_ratio\", \"normalised_flow\", \"shortrun_momentum\"]\n",
    "df_demeaned = demeaning_fund_char(df, id=\"crsp_fundno\", characteristic = char_to_demean)\n",
    "df_demeaned = df_demeaned.drop(columns = char_to_demean)   # drop original columns\n",
    "df.head()\n",
    "\n",
    "df_tech['normalised_flow'] = df_tech['normalised_flow'].fillna(method='ffill')\n",
    "char_to_demean = [\"exp_ratio\", \"turn_ratio\", \"normalised_flow\", \"shortrun_momentum\"]\n",
    "df_tech_demeaned = demeaning_fund_char(df_tech, id=\"crsp_fundno\", characteristic = char_to_demean)\n",
    "df_tech_demeaned = df_tech_demeaned.drop(columns = char_to_demean)   # drop original columns\n",
    "\n",
    "df_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates next date\n",
    "def generate_next_date(list_of_dates, date):\n",
    "    return list_of_dates[list_of_dates > date].min()\n",
    "\n",
    "def process_factor_model(X_factor, y_factor, train_end, test_date):\n",
    "    X_train = X_factor[X_factor['date_L1'] <= train_end].drop(columns='date_L1')\n",
    "    X_test = X_factor[X_factor['date_L1'] == test_date].drop(columns='date_L1')\n",
    "\n",
    "    y_train = y_factor.loc[X_train.index]\n",
    "    y_test = y_factor.loc[X_test.index]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Model Training (Alpha)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asset-Pricing Factor Models:\n",
    "* 5-Factor + MOM Model: mktrf, SMB, HML, RMW, CMA, UMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall_function trains the models using the final selected parameters after hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample code for Regime-Switching Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Training Cycle Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD HYPERPARAMS\n",
    "# lstm_hyperparams_dict_healthcare = dict(\n",
    "#     {'2019': dict({\n",
    "#         'num_layers':3,\n",
    "#         'units':[96,32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.2,0.2,0.1],\n",
    "#         'activation':['tanh','tanh','linear'],\n",
    "#         'lr':0.019962442034576384\n",
    "#     }),\n",
    "#      '2020': dict({\n",
    "#         'num_layers':3,\n",
    "#         'units':[64,32,32],\n",
    "#         'optimizer':'Nadam',\n",
    "#         'drop_out':[0.1,0.1,0.1],\n",
    "#         'activation':['tanh','tanh','linear'],\n",
    "#         'lr':0.021504061608420576\n",
    "#     }),\n",
    "#      '2021': dict({\n",
    "#         'num_layers':3,\n",
    "#         'units':[96,32,32],\n",
    "#         'optimizer':'Nadam',\n",
    "#         'drop_out':[0.1,0.1,0.1],\n",
    "#         'activation':['tanh','tanh','linear'],\n",
    "#         'lr':0.012302249555768368\n",
    "#     }),\n",
    "#      '2022': dict({\n",
    "#         'num_layers':3,\n",
    "#         'units':[96,32,32],\n",
    "#         'optimizer':'Nadam',\n",
    "#         'drop_out':[0.1,0.2,0.2],\n",
    "#         'activation':['tanh','tanh','linear'],\n",
    "#         'lr':0.010092957472413086\n",
    "#     }),\n",
    "#      '2023': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[64,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.2,0.2],\n",
    "#         'activation':['tanh','tanh'],\n",
    "#         'lr':0.018388080425636798\n",
    "#     }),\n",
    "#      '2024': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.1,0.2],\n",
    "#         'activation':['tanh','tanh'],\n",
    "#         'lr':0.007947588699235507\n",
    "#     })}\n",
    "# )\n",
    "\n",
    "rf_hyperparams_dict_healthcare = dict(\n",
    "    {'2019': dict({\n",
    "        'n_estimators':300,\n",
    "        'min_samples_split':5,\n",
    "        'min_samples_leaf':4,\n",
    "        'max_features':'log2',\n",
    "        'max_depth':20\n",
    "    }),\n",
    "     '2020': dict({\n",
    "        'n_estimators':100,\n",
    "        'min_samples_split':10,\n",
    "        'min_samples_leaf':4,\n",
    "        'max_features':'log2',\n",
    "        'max_depth':None\n",
    "    }),\n",
    "     '2021': dict({\n",
    "        'n_estimators':300,\n",
    "        'min_samples_split':5,\n",
    "        'min_samples_leaf':4,\n",
    "        'max_features':'log2',\n",
    "        'max_depth':20\n",
    "    }),\n",
    "     '2022': dict({\n",
    "        'n_estimators':100,\n",
    "        'min_samples_split':10,\n",
    "        'min_samples_leaf':4,\n",
    "        'max_features':'log2',\n",
    "        'max_depth':None\n",
    "    }),\n",
    "     '2023': dict({\n",
    "        'n_estimators':300,\n",
    "        'min_samples_split':5,\n",
    "        'min_samples_leaf':4,\n",
    "        'max_features':'log2',\n",
    "        'max_depth':20\n",
    "    }),\n",
    "     '2024': dict({\n",
    "        'n_estimators':100,\n",
    "        'min_samples_split':10,\n",
    "        'min_samples_leaf':4,\n",
    "        'max_features':'log2',\n",
    "        'max_depth':None\n",
    "    })}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Healthcare Mutual Funds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Define a simplified XGBoost parameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate (step size)\n",
    "    'max_depth': [3, 5, 7],  # Maximum depth of a tree\n",
    "    'subsample': [0.7, 0.8, 0.9],  # Fraction of samples for training each tree\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]  # Fraction of features for each tree\n",
    "}\n",
    "\n",
    "def xgb_tuner(X_train, y_train, xgb_param_grid):\n",
    "    # Initialize XGBRegressor\n",
    "    xgb_model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    xgb_random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=xgb_param_grid,\n",
    "        n_iter=10,  # Number of parameter settings tested\n",
    "        cv=TimeSeriesSplit(n_splits=5),  # TimeSeriesSplit for time-dependent data\n",
    "        scoring='neg_mean_squared_error',  # Evaluate using negative mean squared error\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    xgb_random_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    # Best hyperparameters\n",
    "    best_xgb_params = xgb_random_search.best_params_\n",
    "    print(f'Best XGBoost Parameters: {best_xgb_params}')\n",
    "    return best_xgb_params\n",
    "\n",
    "# Define a simplified LGBM parameter grid\n",
    "lgbm_param_grid = {\n",
    "    'n_estimators': [100, 300, 500],  # Number of boosting rounds\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Learning rate (step size)\n",
    "    'max_depth': [-1, 3, 5],  # Maximum depth of a tree, -1 means no limit\n",
    "    'num_leaves': [31, 63, 127],  # Number of leaves in a tree (controls complexity)\n",
    "    'subsample': [0.7, 0.8, 0.9],  # Fraction of samples used for training each tree\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]  # Fraction of features used for each tree\n",
    "}\n",
    "\n",
    "def lgbm_tuner(X_train, y_train, lgbm_param_grid):\n",
    "    # Initialize LGBMRegressor\n",
    "    lgbm_model = LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    lgbm_random_search = RandomizedSearchCV(\n",
    "        estimator=lgbm_model,\n",
    "        param_distributions=lgbm_param_grid,\n",
    "        n_iter=10,  # Number of parameter settings tested\n",
    "        cv=TimeSeriesSplit(n_splits=5),  # TimeSeriesSplit for time-dependent data\n",
    "        scoring='neg_mean_squared_error',  # Evaluate using negative mean squared error\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    lgbm_random_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    # Best hyperparameters\n",
    "    best_lgbm_params = lgbm_random_search.best_params_\n",
    "    print(f'Best LGBM Parameters: {best_lgbm_params}')\n",
    "    return best_lgbm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cycle\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adagrad\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "def overall_function(dataset, outcome, \n",
    "                     lstm_hyperparams_dict_healthcare, rf_hyperparams_dict_healthcare):\n",
    "    dataset['date'] = pd.to_datetime(dataset['date']) # converting to date format\n",
    "    dataset = dataset.sort_values(by='date')\n",
    "    df_factor = dataset.drop(columns=['mkt_return','mth_return','rf']) # remove irrelevant variables\n",
    "\n",
    "    # Creating Lagged and Stepped Datasets\n",
    "    X_dataset, y_dataset = create_stepped_dataset(create_lagged_dataset(df_factor, lag=1,target_var=outcome, id = 'crsp_fundno'),step=1,target_var=outcome, id = 'crsp_fundno_L1')\n",
    "    # return(X_dataset)\n",
    "    X_dataset = X_dataset.drop(columns=['crsp_fundno_L1'], errors='ignore')\n",
    "\n",
    "    list_of_dates = pd.to_datetime(X_dataset['date_L1'])\n",
    "    percentile_70 = list_of_dates.quantile(0.7) # 70-30 split\n",
    "    train_end = list_of_dates.loc[(list_of_dates - percentile_70).abs().idxmin()]\n",
    "    df_end = list_of_dates.max()\n",
    "    # return(y_dataset)\n",
    "    results = []\n",
    "\n",
    "    ## Implement cross-validation split\n",
    "    tscv = TimeSeriesSplit(n_splits = 5)\n",
    "    \n",
    "    while train_end != df_end:\n",
    "        \n",
    "        test_date = generate_next_date(list_of_dates, train_end)\n",
    "        if pd.isna(test_date):\n",
    "            break \n",
    "\n",
    "        # Process data for modeling\n",
    "        X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "        \n",
    "        # For Adding Results\n",
    "        df_in_loop = y_test.copy()\n",
    "        \n",
    "        ### Model 1: Lasso Regression\n",
    "        lasso_cv = LassoCV(cv = tscv, random_state = 18, max_iter = 100000)\n",
    "        lasso_cv.fit(X_train, y_train)\n",
    "        \n",
    "        # Create the Lasso model with the optimal alpha value\n",
    "        lasso_model = Lasso(alpha = lasso_cv.alpha_)\n",
    "        lasso_model.fit(X_train, y_train)\n",
    "        lassopred = lasso_model.predict(X_test)\n",
    "        # Adding Linear Model\n",
    "        df_in_loop[f'lasso'] = lassopred\n",
    "        print('Lasso Done')\n",
    "        \n",
    "        ### Model 1: Ridge Regression ###\n",
    "        ridge_cv = RidgeCV(cv = tscv)\n",
    "        ridge_cv.fit(X_train, y_train)\n",
    "    \n",
    "        ridge_model = Ridge(alpha = ridge_cv.alpha_)\n",
    "        ridge_model.fit(X_train, y_train)\n",
    "        \n",
    "        ridgepred = ridge_model.predict(X_test)\n",
    "        # Adding Linear Model\n",
    "        df_in_loop[f'ridge'] = ridgepred\n",
    "        print('Ridge Done')\n",
    "        \n",
    "        ### Model 2: LSTM ###\n",
    "        # X_train_lstm = np.array(X_train).astype(np.float32)\n",
    "        # X_test_lstm = np.array(X_test).astype(np.float32)\n",
    "        # y_train_lstm = np.array(y_train).astype(np.float32)\n",
    "        \n",
    "        test_year = test_date.year\n",
    "        # print(f'test_year is {test_year} with hyperparameters of {lstm_hyperparams_dict_healthcare[str(test_year)]}')\n",
    "        # lstm_model = load_model(f'best_lstm_model_{train_end.year}.keras')\n",
    "        \n",
    "        # num_layers = len(lstm_hyperparams_dict_healthcare[str(test_year)]['units'])\n",
    "        \n",
    "        # # Building LSTM Model\n",
    "        # lstm_model = Sequential()\n",
    "        # for layer_num in range(num_layers):\n",
    "        #     is_last_layer = (layer_num == (num_layers - 1))  # Check if it's the last layer\n",
    "        #     lstm_model.add(LSTM(\n",
    "        #         units=lstm_hyperparams_dict_healthcare[str(test_year)]['units'][layer_num], \n",
    "        #         return_sequences=not is_last_layer,  # Only last layer has return_sequences=False\n",
    "        #         input_shape=(X_train_lstm.shape[1], 1) if layer_num == 0 else None,  # Define input shape only for the first layer\n",
    "        #         activation=lstm_hyperparams_dict_healthcare[str(test_year)]['activation'][layer_num]\n",
    "        #     ))\n",
    "        #     lstm_model.add(Dropout(lstm_hyperparams_dict_healthcare[str(test_year)]['drop_out'][layer_num]))\n",
    "        # # Output Layer\n",
    "        # lstm_model.add(Dense(units=1))\n",
    "        \n",
    "        # # Compilation\n",
    "        # if lstm_hyperparams_dict_healthcare[str(test_year)]['optimizer'] == 'Adam':\n",
    "        #     lstm_model.compile(\n",
    "        #         optimizer=Adam(learning_rate=lstm_hyperparams_dict_healthcare[str(test_year)]['lr']),\n",
    "        #         loss='mean_squared_error',\n",
    "        #         metrics=[\n",
    "        #             tf.keras.metrics.RootMeanSquaredError()\n",
    "        #         ]\n",
    "        #     )\n",
    "        # elif lstm_hyperparams_dict_healthcare[str(test_year)]['optimizer'] == 'Nadam':\n",
    "        #     lstm_model.compile(\n",
    "        #         optimizer=Nadam(learning_rate=lstm_hyperparams_dict_healthcare[str(test_year)]['lr']),\n",
    "        #         loss='mean_squared_error',\n",
    "        #         metrics=[\n",
    "        #             tf.keras.metrics.RootMeanSquaredError()\n",
    "        #         ]\n",
    "        #     )\n",
    "\n",
    "        # Early stopping callback\n",
    "        # callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "        # # Train the model\n",
    "        # lstm_model.fit(\n",
    "        #     X_train_lstm, y_train_lstm,\n",
    "        #     epochs=10, batch_size=512,\n",
    "        #     callbacks=[callback]\n",
    "        # )\n",
    "\n",
    "        # lstmpred = lstm_model.predict(X_test_lstm)\n",
    "        # df_in_loop[f'lstm'] = lstmpred\n",
    "        # print('LSTM Done')\n",
    "        \n",
    "        optimal_param_xgb = xgb_tuner(X_train, y_train, xgb_param_grid)\n",
    "        xgboost_model = xgb.XGBRegressor(subsample=optimal_param_xgb['subsample'],\n",
    "                                         n_estimators=optimal_param_xgb['n_estimators'],\n",
    "                                         max_depth=optimal_param_xgb['max_depth'],\n",
    "                                         learning_rate=optimal_param_xgb['learning_rate'],\n",
    "                                         colsample_bytree=optimal_param_xgb['colsample_bytree'],\n",
    "                                         random_state=18)\n",
    "        xgboost_model.fit(X_train, y_train)\n",
    "        df_in_loop[f'xgboost'] = xgboost_model.predict(X_test)\n",
    "        print('XGBoost done')\n",
    "        #errors(xgboost_model, 'gradientboost', X_train, y_train, errors_path, window_end+1, pred_train_path)\n",
    "\n",
    "        ## 16. LightGBM\n",
    "        optimal_param_lgbm = lgbm_tuner(X_train, y_train, lgbm_param_grid)\n",
    "        lightgbm_model = lgb.LGBMRegressor(objective='regression', \n",
    "                                           subsample = optimal_param_lgbm['subsample'],\n",
    "                                           num_leaves = optimal_param_lgbm['num_leaves'],\n",
    "                                           n_estimators = optimal_param_lgbm['n_estimators'],\n",
    "                                           max_depth= optimal_param_lgbm['max_depth'],\n",
    "                                           learning_rate=optimal_param_lgbm['learning_rate'],\n",
    "                                           colsample_bytree=optimal_param_lgbm['colsample_bytree'],\n",
    "                                           random_state=18, verbosity=-1)\n",
    "        lightgbm_model.fit(X_train, y_train)\n",
    "        df_in_loop[f'lgbm'] = lightgbm_model.predict(X_test)\n",
    "        print('LGBM done')\n",
    "                \n",
    "        ### Model 3: Random Forest Regression ###\n",
    "        print(f'test_year is {test_year} with hyperparameters of {rf_hyperparams_dict_healthcare[str(test_year)]}')\n",
    "        rf_model = RandomForestRegressor(n_estimators=rf_hyperparams_dict_healthcare[str(test_year)]['n_estimators'], \n",
    "                                         min_samples_split=rf_hyperparams_dict_healthcare[str(test_year)]['min_samples_split'],\n",
    "                                         min_samples_leaf=rf_hyperparams_dict_healthcare[str(test_year)]['min_samples_leaf'],\n",
    "                                         max_features = rf_hyperparams_dict_healthcare[str(test_year)]['max_features'], \n",
    "                                         max_depth=rf_hyperparams_dict_healthcare[str(test_year)]['max_depth'],\n",
    "                                         random_state=40, n_jobs=-1)\n",
    "\n",
    "        rf_model.fit(X_train, y_train.values.ravel())\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        # return(y_pred)\n",
    "        df_in_loop[f'rf'] = rf_pred\n",
    "        \n",
    "        print('RF Done')\n",
    "        \n",
    "        ### Model 4: PCA ###\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "\n",
    "        X_train_pure = X_train\n",
    "        X_test_pure = X_test\n",
    "        \n",
    "        pca = PCA()\n",
    "        pca.fit(X_train_pure)\n",
    "        cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "          #to explain more than 85% of the variance\n",
    "        num_components = np.where(cumulative_variance_ratio >= 0.85)[0][0] + 1 \n",
    "        pca_new = PCA(n_components=num_components)\n",
    "        X_train_pca = pca_new.fit_transform(X_train_pure)\n",
    "        X_train_pca = pd.DataFrame(X_train_pca)\n",
    "        X_train_pca.columns = X_train_pca.columns.astype(str)\n",
    "\n",
    "        X_test_pca = pca_new.transform(X_test_pure)\n",
    "        X_test_pca = pd.DataFrame(X_test_pca)\n",
    "        X_test_pca.columns = X_test_pca.columns.astype(str)\n",
    "        \n",
    "        pure_factor_model = LinearRegression()\n",
    "        pure_factor_model.fit(X_train_pca, y_train)\n",
    "        y_pred = pure_factor_model.predict(X_test_pca)\n",
    "        df_in_loop[f'pca'] = y_pred\n",
    "        print('PCA Done')\n",
    "        \n",
    "        # return(y_pred)\n",
    "        # Add results into loop\n",
    "        results.append(df_in_loop)\n",
    "        train_end = test_date\n",
    "        num_remaining_dates = len(list(set(date for date in list_of_dates if date > test_date)))\n",
    "        print(f'{num_remaining_dates} dates remaining')\n",
    "    # return(results)\n",
    "    combined_df = pd.concat(results, ignore_index=True)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "57 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "56 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "55 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "54 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "53 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "52 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "51 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "50 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 500, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "49 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "48 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "47 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 500, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "46 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "45 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 500, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "44 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "43 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 500, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "42 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "41 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "40 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 500, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "39 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "38 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "37 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "36 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "35 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "34 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "33 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "32 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "31 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "30 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "29 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "28 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "27 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "26 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "25 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 500, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "24 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "23 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "22 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "21 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "20 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "19 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "18 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "17 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "16 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "15 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "14 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 500, 'max_depth': -1, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "13 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "12 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "11 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "10 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "9 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "8 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "7 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "6 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "5 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "4 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "3 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "2 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "1 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "0 dates remaining\n"
     ]
    }
   ],
   "source": [
    "# Run to train the models\n",
    "y_new = overall_function(dataset=df, outcome = \"rolling_alpha_5f\", \n",
    "                         lstm_hyperparams_dict_healthcare=None, rf_hyperparams_dict_healthcare=rf_hyperparams_dict_healthcare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new.to_csv('tuned_results_xgboostlgbm_yearly_tuned_every_round.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technology Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_hyperparams_dict_tech = dict(\n",
    "#     {'2019': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.1,0.1],\n",
    "#         'activation':['linear','tanh'],\n",
    "#         'lr':0.04130929884537057\n",
    "#     }),\n",
    "#      '2020': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[96,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.1,0.1],\n",
    "#         'activation':['tanh','tanh'],\n",
    "#         'lr':0.0017545326289340611\n",
    "#     }),\n",
    "#      '2021': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[96,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.1,0.2],\n",
    "#         'activation':['linear','tanh'],\n",
    "#         'lr':0.008514542675036612\n",
    "#     }),\n",
    "#      '2022': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[32,32],\n",
    "#         'optimizer':'Adagrad',\n",
    "#         'drop_out':[0.2,0.1],\n",
    "#         'activation':['linear','tanh'],\n",
    "#         'lr':0.06210952184736157\n",
    "#     }),\n",
    "#      '2023': dict({\n",
    "#         'num_layers':4,\n",
    "#         'units':[128,32,32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.2,0.1,0.1,0.2],\n",
    "#         'activation':['tanh','tanh','linear','linear'],\n",
    "#         'lr':0.019671144721368838\n",
    "#     }),\n",
    "#      '2024': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.2,0.1],\n",
    "#         'activation':['tanh','tanh'],\n",
    "#         'lr':0.004409350423921994\n",
    "#     })}\n",
    "# )\n",
    "lstm_hyperparams_dict_tech = dict(\n",
    "    {'2019': dict({\n",
    "        'num_layers':2,\n",
    "        'units':[32,32],\n",
    "        'optimizer':'Adam',\n",
    "        'drop_out':[0.1,0.1],\n",
    "        'activation':['linear','tanh'],\n",
    "        'lr':0.04130929884537057\n",
    "    }),\n",
    "     '2020': dict({\n",
    "        'num_layers':2,\n",
    "        'units':[96,32],\n",
    "        'optimizer':'Adam',\n",
    "        'drop_out':[0.1,0.1],\n",
    "        'activation':['tanh','tanh'],\n",
    "        'lr':0.0017545326289340611\n",
    "    }),\n",
    "     '2021': dict({\n",
    "        'num_layers':2,\n",
    "        'units':[96,32],\n",
    "        'optimizer':'Adam',\n",
    "        'drop_out':[0.1,0.2],\n",
    "        'activation':['linear','tanh'],\n",
    "        'lr':0.008514542675036612\n",
    "    }),\n",
    "     '2022': dict({\n",
    "        'num_layers':2,\n",
    "        'units':[32,32],\n",
    "        'optimizer':'Adagrad',\n",
    "        'drop_out':[0.2,0.1],\n",
    "        'activation':['linear','tanh'],\n",
    "        'lr':0.06210952184736157\n",
    "    }),\n",
    "     '2023': dict({\n",
    "        'num_layers':4,\n",
    "        'units':[128,32,32,32],\n",
    "        'optimizer':'Adam',\n",
    "        'drop_out':[0.2,0.1,0.1,0.2],\n",
    "        'activation':['tanh','tanh','linear','linear'],\n",
    "        'lr':0.019671144721368838\n",
    "    }),\n",
    "     '2024': dict({\n",
    "        'num_layers':2,\n",
    "        'units':[32,32],\n",
    "        'optimizer':'Adam',\n",
    "        'drop_out':[0.2,0.1],\n",
    "        'activation':['tanh','tanh'],\n",
    "        'lr':0.004409350423921994\n",
    "    })}\n",
    ")\n",
    "\n",
    "rf_hyperparams_dict_tech = dict(\n",
    "    {'2019': dict({\n",
    "        'n_estimators': 300,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': 'log2',\n",
    "        'max_depth': 20\n",
    "    }),\n",
    "     '2020': dict({\n",
    "        'n_estimators': 300,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': 'log2',\n",
    "        'max_depth': 20\n",
    "    }),\n",
    "     '2021': dict({\n",
    "        'n_estimators': 300,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': 'log2',\n",
    "        'max_depth': 20\n",
    "    }),\n",
    "     '2022': dict({\n",
    "        'n_estimators': 100,\n",
    "        'min_samples_split': 10,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': 'log2',\n",
    "        'max_depth': None\n",
    "    }),\n",
    "     '2023': dict({\n",
    "        'n_estimators': 300,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': 'log2',\n",
    "        'max_depth': 20\n",
    "    }),\n",
    "     '2024': dict({\n",
    "        'n_estimators': 100,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': 'log2',\n",
    "        'max_depth': 30\n",
    "    })}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "def overall_function(dataset, outcome,lstm_hyperparams_dict_tech, rf_hyperparams_dict_tech):\n",
    "    dataset['date'] = pd.to_datetime(dataset['date']) # converting to date format\n",
    "    dataset = dataset.sort_values(by='date')\n",
    "    df_factor = dataset.drop(columns=['mkt_return','mth_return','rf','rolling_sharpe', 'rolling_alpha_3f', 'rolling_alpha_4f']) # remove irrelevant variables\n",
    "\n",
    "    # Creating Lagged and Stepped Datasets\n",
    "    X_dataset, y_dataset = create_stepped_dataset(create_lagged_dataset(df_factor, lag=1,target_var=outcome, id = 'crsp_fundno'),step=1,target_var=outcome, id = 'crsp_fundno_L1')\n",
    "    # return(X_dataset)\n",
    "    X_dataset = X_dataset.drop(columns=['crsp_fundno_L1'], errors='ignore')\n",
    "\n",
    "    list_of_dates = pd.to_datetime(X_dataset['date_L1'])\n",
    "    percentile_70 = list_of_dates.quantile(0.7) # 70-30 split\n",
    "    train_end = list_of_dates.loc[(list_of_dates - percentile_70).abs().idxmin()]\n",
    "    df_end = list_of_dates.max()\n",
    "    # return(y_dataset)\n",
    "    results = []\n",
    "\n",
    "    ## Implement cross-validation split\n",
    "    tscv = TimeSeriesSplit(n_splits = 5)\n",
    "    \n",
    "    while train_end != df_end:\n",
    "        \n",
    "        test_date = generate_next_date(list_of_dates, train_end)\n",
    "        if pd.isna(test_date):\n",
    "            break \n",
    "\n",
    "        # Process data for modeling\n",
    "        X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "        \n",
    "        # For Adding Results\n",
    "        df_in_loop = y_test.copy()\n",
    "        \n",
    "        ### Model 1: Lasso Regression\n",
    "        lasso_cv = LassoCV(cv = tscv, random_state = 18, max_iter = 100000)\n",
    "        lasso_cv.fit(X_train, y_train)\n",
    "        \n",
    "        # Create the Lasso model with the optimal alpha value\n",
    "        lasso_model = Lasso(alpha = lasso_cv.alpha_)\n",
    "        lasso_model.fit(X_train, y_train)\n",
    "        lassopred = lasso_model.predict(X_test)\n",
    "        # Adding Linear Model\n",
    "        df_in_loop[f'lasso'] = lassopred\n",
    "        print('Lasso Done')\n",
    "        \n",
    "        ### Model 1: Ridge Regression ###\n",
    "        ridge_cv = RidgeCV(cv = tscv)\n",
    "        ridge_cv.fit(X_train, y_train)\n",
    "    \n",
    "        ridge_model = Ridge(alpha = ridge_cv.alpha_)\n",
    "        ridge_model.fit(X_train, y_train)\n",
    "        \n",
    "        ridgepred = ridge_model.predict(X_test)\n",
    "        # Adding Linear Model\n",
    "        df_in_loop[f'ridge'] = ridgepred\n",
    "        print('Ridge Done')\n",
    "        \n",
    "        ### Model 2: LSTM ###\n",
    "        X_train_lstm = np.array(X_train).astype(np.float32)\n",
    "        X_test_lstm = np.array(X_test).astype(np.float32)\n",
    "        y_train_lstm = np.array(y_train).astype(np.float32)\n",
    "        \n",
    "        test_year = test_date.year\n",
    "        print(f'test_year is {test_year} with hyperparameters of {lstm_hyperparams_dict_tech[str(test_year)]}')\n",
    "        # lstm_model = load_model(f'best_lstm_model_{train_end.year}.keras')\n",
    "        \n",
    "        num_layers = len(lstm_hyperparams_dict_tech[str(test_year)]['units'])\n",
    "        \n",
    "        # Building LSTM Model\n",
    "        lstm_model = Sequential()\n",
    "        for layer_num in range(num_layers):\n",
    "            is_last_layer = (layer_num == (num_layers - 1))  # Check if it's the last layer\n",
    "            lstm_model.add(LSTM(\n",
    "                units=lstm_hyperparams_dict_tech[str(test_year)]['units'][layer_num], \n",
    "                return_sequences=not is_last_layer,  # Only last layer has return_sequences=False\n",
    "                input_shape=(X_train_lstm.shape[1], 1) if layer_num == 0 else None,  # Define input shape only for the first layer\n",
    "                activation=lstm_hyperparams_dict_tech[str(test_year)]['activation'][layer_num]\n",
    "            ))\n",
    "            lstm_model.add(Dropout(lstm_hyperparams_dict_tech[str(test_year)]['drop_out'][layer_num]))\n",
    "        # Output Layer\n",
    "        lstm_model.add(Dense(units=1))\n",
    "        \n",
    "        # Compilation\n",
    "        if lstm_hyperparams_dict_tech[str(test_year)]['optimizer'] == 'Adam':\n",
    "            lstm_model.compile(\n",
    "                optimizer=Adam(learning_rate=lstm_hyperparams_dict_tech[str(test_year)]['lr']),\n",
    "                loss='mean_squared_error',\n",
    "                metrics=[\n",
    "                    tf.keras.metrics.RootMeanSquaredError()\n",
    "                ]\n",
    "            )\n",
    "        elif lstm_hyperparams_dict_tech[str(test_year)]['optimizer'] == 'Nadam':\n",
    "            lstm_model.compile(\n",
    "                optimizer=Nadam(learning_rate=lstm_hyperparams_dict_tech[str(test_year)]['lr']),\n",
    "                loss='mean_squared_error',\n",
    "                metrics=[\n",
    "                    tf.keras.metrics.RootMeanSquaredError()\n",
    "                ]\n",
    "            )\n",
    "        elif lstm_hyperparams_dict_tech[str(test_year)]['optimizer'] == 'Adagrad':\n",
    "            lstm_model.compile(\n",
    "                optimizer=Adagrad(learning_rate=lstm_hyperparams_dict_tech[str(test_year)]['lr']),\n",
    "                loss='mean_squared_error',\n",
    "                metrics=[\n",
    "                    tf.keras.metrics.RootMeanSquaredError()\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Early stopping callback\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "        # Train the model\n",
    "        lstm_model.fit(\n",
    "            X_train_lstm, y_train_lstm,\n",
    "            epochs=10, batch_size=512,\n",
    "            callbacks=[callback]\n",
    "        )\n",
    "\n",
    "        lstmpred = lstm_model.predict(X_test_lstm)\n",
    "        df_in_loop[f'lstm'] = lstmpred\n",
    "        print('LSTM Done')\n",
    "        \n",
    "        optimal_param_xgb = xgb_tuner(X_train, y_train, xgb_param_grid)\n",
    "        xgboost_model = xgb.XGBRegressor(subsample=optimal_param_xgb['subsample'],\n",
    "                                         n_estimators=optimal_param_xgb['n_estimators'],\n",
    "                                         max_depth=optimal_param_xgb['max_depth'],\n",
    "                                         learning_rate=optimal_param_xgb['learning_rate'],\n",
    "                                         colsample_bytree=optimal_param_xgb['colsample_bytree'],\n",
    "                                         random_state=18)\n",
    "        xgboost_model.fit(X_train, y_train)\n",
    "        df_in_loop[f'xgboost'] = xgboost_model.predict(X_test)\n",
    "        print('XGBoost done')\n",
    "\n",
    "        ## 16. LightGBM\n",
    "        optimal_param_lgbm = lgbm_tuner(X_train, y_train, lgbm_param_grid)\n",
    "        lightgbm_model = lgb.LGBMRegressor(objective='regression', \n",
    "                                           subsample = optimal_param_lgbm['subsample'],\n",
    "                                           num_leaves = optimal_param_lgbm['num_leaves'],\n",
    "                                           n_estimators = optimal_param_lgbm['n_estimators'],\n",
    "                                           max_depth= optimal_param_lgbm['max_depth'],\n",
    "                                           learning_rate=optimal_param_lgbm['learning_rate'],\n",
    "                                           colsample_bytree=optimal_param_lgbm['colsample_bytree'],\n",
    "                                           random_state=18, verbosity=-1)\n",
    "        lightgbm_model.fit(X_train, y_train)\n",
    "        df_in_loop[f'lgbm'] = lightgbm_model.predict(X_test)\n",
    "        print('LGBM done')\n",
    "                \n",
    "        ### Model 3: Random Forest Regression ###\n",
    "        print(f'test_year is {test_year} with hyperparameters of {rf_hyperparams_dict_tech[str(test_year)]}')\n",
    "        rf_model = RandomForestRegressor(n_estimators=rf_hyperparams_dict_tech[str(test_year)]['n_estimators'], \n",
    "                                         min_samples_split=rf_hyperparams_dict_tech[str(test_year)]['min_samples_split'],\n",
    "                                         min_samples_leaf=rf_hyperparams_dict_tech[str(test_year)]['min_samples_leaf'],\n",
    "                                         max_features = rf_hyperparams_dict_tech[str(test_year)]['max_features'], \n",
    "                                         max_depth=rf_hyperparams_dict_tech[str(test_year)]['max_depth'],\n",
    "                                         random_state=40, n_jobs=-1)\n",
    "\n",
    "        rf_model.fit(X_train, y_train.values.ravel())\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        # return(y_pred)\n",
    "        df_in_loop[f'rf'] = rf_pred\n",
    "        \n",
    "        print('RF Done')\n",
    "        \n",
    "        ### Model 5: PCA ###\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "\n",
    "        X_train_pure = X_train\n",
    "        X_test_pure = X_test\n",
    "        \n",
    "        pca = PCA()\n",
    "        pca.fit(X_train_pure)\n",
    "        cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "          #to explain more than 85% of the variance\n",
    "        num_components = np.where(cumulative_variance_ratio >= 0.85)[0][0] + 1 \n",
    "        pca_new = PCA(n_components=num_components)\n",
    "        X_train_pca = pca_new.fit_transform(X_train_pure)\n",
    "        X_train_pca = pd.DataFrame(X_train_pca)\n",
    "        X_train_pca.columns = X_train_pca.columns.astype(str)\n",
    "\n",
    "        X_test_pca = pca_new.transform(X_test_pure)\n",
    "        X_test_pca = pd.DataFrame(X_test_pca)\n",
    "        X_test_pca.columns = X_test_pca.columns.astype(str)\n",
    "        \n",
    "        pure_factor_model = LinearRegression()\n",
    "        pure_factor_model.fit(X_train_pca, y_train)\n",
    "        y_pred = pure_factor_model.predict(X_test_pca)\n",
    "        df_in_loop[f'pca'] = y_pred\n",
    "        print('PCA Done')\n",
    "        \n",
    "        # Add results into loop\n",
    "        results.append(df_in_loop)\n",
    "        train_end = test_date\n",
    "        num_remaining_dates = len(list(set(date for date in list_of_dates if date > test_date)))\n",
    "        print(f'{num_remaining_dates} dates remaining')\n",
    "        \n",
    "    combined_df = pd.concat(results, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - loss: 0.0207 - root_mean_squared_error: 0.1307\n",
      "Epoch 2/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0010 - root_mean_squared_error: 0.0303    \n",
      "Epoch 3/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0035 - root_mean_squared_error: 0.0580\n",
      "Epoch 4/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 6.1520e-04 - root_mean_squared_error: 0.0248\n",
      "Epoch 5/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 6.1096e-04 - root_mean_squared_error: 0.0247\n",
      "Epoch 6/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 5.9978e-04 - root_mean_squared_error: 0.0245\n",
      "Epoch 7/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 5.9395e-04 - root_mean_squared_error: 0.0244\n",
      "Epoch 8/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: 5.6909e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 9/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 6.5562e-04 - root_mean_squared_error: 0.0256\n",
      "Epoch 10/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 6.7221e-04 - root_mean_squared_error: 0.0259\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step \n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4229\n",
      "[LightGBM] [Info] Number of data points in the train set: 38150, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.003498\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "64 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 39ms/step - loss: 0.0225 - root_mean_squared_error: 0.1342\n",
      "Epoch 2/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.6206e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 3/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.5059e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 4/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.6153e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 5/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.0531e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 6/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 4.9360e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 7/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.0816e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 8/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.1830e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 9/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 4.8840e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 10/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 0.0013 - root_mean_squared_error: 0.0304\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step \n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "63 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - loss: 0.0144 - root_mean_squared_error: 0.1084\n",
      "Epoch 2/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 5.6547e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 3/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 5.3872e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 4/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.2007e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 5/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 5.1662e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 6/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.2717e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 7/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 5.0290e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 8/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 5.2412e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 9/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 5.1174e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 10/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 4.9238e-04 - root_mean_squared_error: 0.0222\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "62 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 39ms/step - loss: 0.0116 - root_mean_squared_error: 0.0970\n",
      "Epoch 2/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.4368e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 3/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.4776e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 4/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.3893e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 5/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 5.1502e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 6/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 4.9556e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 7/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.0762e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 8/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.2898e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 9/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 4.7216e-04 - root_mean_squared_error: 0.0217\n",
      "Epoch 10/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 4.9111e-04 - root_mean_squared_error: 0.0222\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "61 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 39ms/step - loss: 0.0277 - root_mean_squared_error: 0.1482\n",
      "Epoch 2/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.7339e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 3/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.4382e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 4/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.0213e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 5/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.3706e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 6/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 5.2027e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 7/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.1909e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 8/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.1327e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 9/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.1074e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 10/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 4.7289e-04 - root_mean_squared_error: 0.0217\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C751C3E320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "60 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.0243 - root_mean_squared_error: 0.1417\n",
      "Epoch 2/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0012 - root_mean_squared_error: 0.0339\n",
      "Epoch 3/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 6.5324e-04 - root_mean_squared_error: 0.0255\n",
      "Epoch 4/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 6.1298e-04 - root_mean_squared_error: 0.0248\n",
      "Epoch 5/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.9429e-04 - root_mean_squared_error: 0.0244\n",
      "Epoch 6/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 5.7073e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 7/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 6.0405e-04 - root_mean_squared_error: 0.0246\n",
      "Epoch 8/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: 6.0754e-04 - root_mean_squared_error: 0.0246\n",
      "Epoch 9/10\n",
      "\u001b[1m77/77\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 6.2310e-04 - root_mean_squared_error: 0.0250\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C75705C430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "59 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - loss: 0.0334 - root_mean_squared_error: 0.1633\n",
      "Epoch 2/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 6.1536e-04 - root_mean_squared_error: 0.0248\n",
      "Epoch 3/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.9633e-04 - root_mean_squared_error: 0.0244\n",
      "Epoch 4/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0030 - root_mean_squared_error: 0.0446\n",
      "Epoch 5/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0053 - root_mean_squared_error: 0.0709\n",
      "Epoch 6/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 6.3141e-04 - root_mean_squared_error: 0.0251\n",
      "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "58 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.0100 - root_mean_squared_error: 0.0908\n",
      "Epoch 2/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.4284e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 3/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 5.1177e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 4/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.3442e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 5/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 4.6988e-04 - root_mean_squared_error: 0.0217\n",
      "Epoch 6/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 4.7877e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 7/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0015 - root_mean_squared_error: 0.0291  \n",
      "Epoch 8/10\n",
      "\u001b[1m78/78\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 0.0549 - root_mean_squared_error: 0.2242\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "57 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - loss: 0.0246 - root_mean_squared_error: 0.1410\n",
      "Epoch 2/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.4822e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 3/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.6633e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 4/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.3611e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 5/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.0671e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 6/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.9882e-04 - root_mean_squared_error: 0.0245\n",
      "Epoch 7/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.1138e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 8/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.2682e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 9/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.8337e-04 - root_mean_squared_error: 0.0241\n",
      "Epoch 10/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 4.9220e-04 - root_mean_squared_error: 0.0222\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "56 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 39ms/step - loss: 0.0239 - root_mean_squared_error: 0.1384\n",
      "Epoch 2/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.2579e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 3/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 4.9225e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 4/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.2771e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 5/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.3643e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 6/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 4.9413e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 7/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 4.8918e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 8/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 4.8911e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 9/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.2004e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 10/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 4.7061e-04 - root_mean_squared_error: 0.0217\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "55 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2019 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.04130929884537057}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - loss: 0.0193 - root_mean_squared_error: 0.1240\n",
      "Epoch 2/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.4249e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 3/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.1909e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 4/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.3225e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 5/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.2654e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 6/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.0504e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 7/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 5.3054e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 8/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: 5.0405e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 9/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 4.8301e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 10/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 4.5658e-04 - root_mean_squared_error: 0.0214\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2019 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "54 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 75ms/step - loss: 7.1512e-04 - root_mean_squared_error: 0.0266\n",
      "Epoch 2/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.2293e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 3/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.0792e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 4/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.3969e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 5/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 5.2361e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 6/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.1758e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 7/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 4.8131e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 8/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 4.9358e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 9/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 77ms/step - loss: 5.1661e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 10/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.2763e-04 - root_mean_squared_error: 0.0229\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n",
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "53 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - loss: 7.7106e-04 - root_mean_squared_error: 0.0276\n",
      "Epoch 2/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.1458e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 3/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.2171e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 4/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.0608e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 5/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.0334e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 6/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.3456e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 7/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.1940e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 8/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 4.8553e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 9/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 5.0131e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 10/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 4.9127e-04 - root_mean_squared_error: 0.0222\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "52 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 73ms/step - loss: 9.9674e-04 - root_mean_squared_error: 0.0312\n",
      "Epoch 2/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.6618e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 3/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.5250e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 4/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.1115e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 5/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.6857e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 6/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.3999e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 7/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.1053e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 8/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.3118e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 9/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.1915e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 10/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 5.2936e-04 - root_mean_squared_error: 0.0230\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "51 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 74ms/step - loss: 0.0010 - root_mean_squared_error: 0.0317\n",
      "Epoch 2/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.7919e-04 - root_mean_squared_error: 0.0241\n",
      "Epoch 3/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.7281e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 4/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.5987e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 5/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.7249e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 6/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.5666e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 7/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.3046e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 8/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.4176e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 9/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.1878e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 10/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.5283e-04 - root_mean_squared_error: 0.0235\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "50 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 78ms/step - loss: 7.6604e-04 - root_mean_squared_error: 0.0275\n",
      "Epoch 2/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 5.7275e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 3/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 5.4671e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 4/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.5460e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 5/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.4637e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 6/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.5408e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 7/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 78ms/step - loss: 5.2549e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 8/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 5.5428e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 9/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.1216e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 10/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 4.9571e-04 - root_mean_squared_error: 0.0223\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "49 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 75ms/step - loss: 7.6515e-04 - root_mean_squared_error: 0.0275\n",
      "Epoch 2/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.5844e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 3/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.8848e-04 - root_mean_squared_error: 0.0243\n",
      "Epoch 4/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.4716e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 5/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.5351e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 6/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.8343e-04 - root_mean_squared_error: 0.0241\n",
      "Epoch 7/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - loss: 5.5395e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 8/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.2316e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 9/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.1350e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 10/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.2712e-04 - root_mean_squared_error: 0.0230\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "48 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 75ms/step - loss: 9.0910e-04 - root_mean_squared_error: 0.0299\n",
      "Epoch 2/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.7472e-04 - root_mean_squared_error: 0.0240\n",
      "Epoch 3/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.5336e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 4/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.4026e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 5/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.4891e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 6/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.8477e-04 - root_mean_squared_error: 0.0242\n",
      "Epoch 7/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.5191e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 8/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.4603e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 9/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.0720e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 10/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.3308e-04 - root_mean_squared_error: 0.0231\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "47 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 74ms/step - loss: 8.5279e-04 - root_mean_squared_error: 0.0289\n",
      "Epoch 2/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.7006e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 3/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.6384e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 4/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.6767e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 5/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.5431e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 6/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step - loss: 5.6755e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 7/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 76ms/step - loss: 5.1569e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 8/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step - loss: 5.4094e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 9/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.1970e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 10/10\n",
      "\u001b[1m84/84\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 5.3631e-04 - root_mean_squared_error: 0.0232\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "46 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 73ms/step - loss: 0.0012 - root_mean_squared_error: 0.0335\n",
      "Epoch 2/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.5781e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 3/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.4430e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 4/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.6062e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 5/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.6922e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 6/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.7311e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 7/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.2969e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 8/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.3233e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 9/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.5458e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 10/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.4714e-04 - root_mean_squared_error: 0.0234\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "45 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 73ms/step - loss: 7.6221e-04 - root_mean_squared_error: 0.0274\n",
      "Epoch 2/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.6518e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 3/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.6348e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 4/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.5815e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 5/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.3387e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 6/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.0944e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 7/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step - loss: 5.0851e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 8/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - loss: 5.2086e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 9/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - loss: 5.4260e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 10/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - loss: 5.0266e-04 - root_mean_squared_error: 0.0224\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "44 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 71ms/step - loss: 9.6749e-04 - root_mean_squared_error: 0.0307\n",
      "Epoch 2/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.6910e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 3/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 5.5334e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 4/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.8690e-04 - root_mean_squared_error: 0.0242\n",
      "Epoch 5/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.4091e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 6/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.3359e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 7/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.7298e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 8/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.2944e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 9/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.3213e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 10/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.2430e-04 - root_mean_squared_error: 0.0229\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "43 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2020 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.0017545326289340611}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 74ms/step - loss: 8.4769e-04 - root_mean_squared_error: 0.0289\n",
      "Epoch 2/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.3318e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 3/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.6912e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 4/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.4769e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 5/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.6228e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 6/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.7292e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 7/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.2740e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 8/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 5.5642e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 9/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.2642e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 10/10\n",
      "\u001b[1m86/86\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.5798e-04 - root_mean_squared_error: 0.0236\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.7, 'num_leaves': 31, 'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2020 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "42 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 70ms/step - loss: 0.0049 - root_mean_squared_error: 0.0635\n",
      "Epoch 2/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.8245e-04 - root_mean_squared_error: 0.0241\n",
      "Epoch 3/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.5775e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 4/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.5413e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 5/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.4420e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 6/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.2638e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 7/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.5471e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 8/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.7038e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 9/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.4250e-04 - root_mean_squared_error: 0.0233\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "41 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 70ms/step - loss: 0.0048 - root_mean_squared_error: 0.0630\n",
      "Epoch 2/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 5.7017e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 3/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.3654e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 4/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.5429e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 5/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.4354e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 6/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 5.6824e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 7/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.6242e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 8/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 0.0015 - root_mean_squared_error: 0.0359\n",
      "Epoch 9/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 0.0015 - root_mean_squared_error: 0.0375\n",
      "Epoch 10/10\n",
      "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 6.0371e-04 - root_mean_squared_error: 0.0246\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "40 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 70ms/step - loss: 0.0055 - root_mean_squared_error: 0.0673\n",
      "Epoch 2/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.6833e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 3/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.8387e-04 - root_mean_squared_error: 0.0241\n",
      "Epoch 4/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 0.0044 - root_mean_squared_error: 0.0597\n",
      "Epoch 5/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 6.3776e-04 - root_mean_squared_error: 0.0252\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "39 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 69ms/step - loss: 0.0037 - root_mean_squared_error: 0.0557\n",
      "Epoch 2/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.6401e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 3/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.7113e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 4/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.3299e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 5/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.4994e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 6/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 5.4093e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 7/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.4720e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 8/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.4614e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 9/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.3388e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 10/10\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.1241e-04 - root_mean_squared_error: 0.0226\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "38 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 71ms/step - loss: 0.0055 - root_mean_squared_error: 0.0672\n",
      "Epoch 2/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.8581e-04 - root_mean_squared_error: 0.0242\n",
      "Epoch 3/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.6976e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 4/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 7.5407e-04 - root_mean_squared_error: 0.0273\n",
      "Epoch 5/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 6.5753e-04 - root_mean_squared_error: 0.0256\n",
      "Epoch 6/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 6.5957e-04 - root_mean_squared_error: 0.0257\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "37 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step - loss: 0.0028 - root_mean_squared_error: 0.0490\n",
      "Epoch 2/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.5546e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 3/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.7256e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 4/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.2973e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 5/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.2949e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 6/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.5859e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 7/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.3078e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 8/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.3781e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 9/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.1641e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 10/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.3396e-04 - root_mean_squared_error: 0.0231\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "36 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 70ms/step - loss: 0.0057 - root_mean_squared_error: 0.0681\n",
      "Epoch 2/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.3125e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 3/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.3968e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 4/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.3277e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 5/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.5375e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 6/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.4925e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 7/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.0770e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 8/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.2504e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 9/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.1261e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 10/10\n",
      "\u001b[1m90/90\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.2780e-04 - root_mean_squared_error: 0.0230\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "35 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step - loss: 0.0037 - root_mean_squared_error: 0.0556\n",
      "Epoch 2/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.6841e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 3/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.4780e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 4/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.5369e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 5/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.4907e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 6/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - loss: 5.3096e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 7/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.4357e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 8/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - loss: 5.1634e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 9/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 68ms/step - loss: 5.2599e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 10/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 5.0791e-04 - root_mean_squared_error: 0.0225\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Done\n",
      "PCA Done\n",
      "34 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step - loss: 0.0051 - root_mean_squared_error: 0.0650\n",
      "Epoch 2/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 6.6811e-04 - root_mean_squared_error: 0.0258\n",
      "Epoch 3/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.6957e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 4/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 7.3906e-04 - root_mean_squared_error: 0.0272\n",
      "Epoch 5/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 69ms/step - loss: 0.0019 - root_mean_squared_error: 0.0383\n",
      "Epoch 6/10\n",
      "\u001b[1m91/91\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 0.0026 - root_mean_squared_error: 0.0492\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "33 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step - loss: 0.0065 - root_mean_squared_error: 0.0726\n",
      "Epoch 2/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 5.6313e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 3/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 5.3626e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 4/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 5.2635e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 5/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 5.5330e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 6/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 5.6087e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 7/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 5.7336e-04 - root_mean_squared_error: 0.0239\n",
      "Epoch 8/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 5.2432e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 9/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 5.3902e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 10/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 4.9974e-04 - root_mean_squared_error: 0.0223\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "32 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 70ms/step - loss: 0.0038 - root_mean_squared_error: 0.0568\n",
      "Epoch 2/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 5.6720e-04 - root_mean_squared_error: 0.0238\n",
      "Epoch 3/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 70ms/step - loss: 5.5839e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 4/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 5.2050e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 5/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - loss: 5.5785e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 6/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - loss: 5.3938e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 7/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - loss: 5.5843e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 8/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - loss: 5.5960e-04 - root_mean_squared_error: 0.0237\n",
      "Epoch 9/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - loss: 0.0101 - root_mean_squared_error: 0.0883\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "31 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2021 with hyperparameters of {'num_layers': 2, 'units': [96, 32], 'optimizer': 'Adam', 'drop_out': [0.1, 0.2], 'activation': ['linear', 'tanh'], 'lr': 0.008514542675036612}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 71ms/step - loss: 0.0032 - root_mean_squared_error: 0.0518\n",
      "Epoch 2/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 5.3031e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 3/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - loss: 5.2981e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 4/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 5.1816e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 5/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 5.5294e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 6/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 5.4173e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 7/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 5.3465e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 8/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 5.2485e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 9/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 5.0770e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 10/10\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 5.0019e-04 - root_mean_squared_error: 0.0224\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2021 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "30 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - loss: 0.0017 - root_mean_squared_error: 0.0402\n",
      "Epoch 2/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 8.6942e-04 - root_mean_squared_error: 0.0295\n",
      "Epoch 3/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 7.6399e-04 - root_mean_squared_error: 0.0276\n",
      "Epoch 4/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 7.8825e-04 - root_mean_squared_error: 0.0281\n",
      "Epoch 5/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 7.3270e-04 - root_mean_squared_error: 0.0271\n",
      "Epoch 6/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 7.3094e-04 - root_mean_squared_error: 0.0270\n",
      "Epoch 7/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 7.2131e-04 - root_mean_squared_error: 0.0269\n",
      "Epoch 8/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.9575e-04 - root_mean_squared_error: 0.0264\n",
      "Epoch 9/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 7.1458e-04 - root_mean_squared_error: 0.0267\n",
      "Epoch 10/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 7.0788e-04 - root_mean_squared_error: 0.0266\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "29 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 39ms/step - loss: 9.1077e-04 - root_mean_squared_error: 0.0302\n",
      "Epoch 2/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 7.6514e-04 - root_mean_squared_error: 0.0277\n",
      "Epoch 3/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 7.3913e-04 - root_mean_squared_error: 0.0272\n",
      "Epoch 4/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 7.0439e-04 - root_mean_squared_error: 0.0265\n",
      "Epoch 5/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 6.8566e-04 - root_mean_squared_error: 0.0262\n",
      "Epoch 6/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 6.6650e-04 - root_mean_squared_error: 0.0258\n",
      "Epoch 7/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 6.8898e-04 - root_mean_squared_error: 0.0262\n",
      "Epoch 8/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 6.3610e-04 - root_mean_squared_error: 0.0252\n",
      "Epoch 9/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 6.5177e-04 - root_mean_squared_error: 0.0255\n",
      "Epoch 10/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 6.4445e-04 - root_mean_squared_error: 0.0254\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.8, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "28 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - loss: 8.8803e-04 - root_mean_squared_error: 0.0298\n",
      "Epoch 2/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - loss: 8.0077e-04 - root_mean_squared_error: 0.0283\n",
      "Epoch 3/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m814s\u001b[0m 9s/step - loss: 7.5485e-04 - root_mean_squared_error: 0.0275\n",
      "Epoch 4/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - loss: 7.3808e-04 - root_mean_squared_error: 0.0272\n",
      "Epoch 5/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 7.2371e-04 - root_mean_squared_error: 0.0269\n",
      "Epoch 6/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 6.9602e-04 - root_mean_squared_error: 0.0264\n",
      "Epoch 7/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 6.6057e-04 - root_mean_squared_error: 0.0257\n",
      "Epoch 8/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 6.6696e-04 - root_mean_squared_error: 0.0258\n",
      "Epoch 9/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 6.4977e-04 - root_mean_squared_error: 0.0255\n",
      "Epoch 10/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 6.3790e-04 - root_mean_squared_error: 0.0253\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "27 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - loss: 8.3985e-04 - root_mean_squared_error: 0.0290\n",
      "Epoch 2/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 7.6321e-04 - root_mean_squared_error: 0.0276\n",
      "Epoch 3/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.9873e-04 - root_mean_squared_error: 0.0264\n",
      "Epoch 4/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 7.2092e-04 - root_mean_squared_error: 0.0268\n",
      "Epoch 5/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.8386e-04 - root_mean_squared_error: 0.0261\n",
      "Epoch 6/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 6.8664e-04 - root_mean_squared_error: 0.0262\n",
      "Epoch 7/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.9968e-04 - root_mean_squared_error: 0.0264\n",
      "Epoch 8/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - loss: 6.3980e-04 - root_mean_squared_error: 0.0253\n",
      "Epoch 9/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.4571e-04 - root_mean_squared_error: 0.0254\n",
      "Epoch 10/10\n",
      "\u001b[1m95/95\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 6.3007e-04 - root_mean_squared_error: 0.0251\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "26 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - loss: 0.0019 - root_mean_squared_error: 0.0431\n",
      "Epoch 2/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 8.6198e-04 - root_mean_squared_error: 0.0294\n",
      "Epoch 3/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 7.9063e-04 - root_mean_squared_error: 0.0281\n",
      "Epoch 4/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 7.7082e-04 - root_mean_squared_error: 0.0278\n",
      "Epoch 5/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 7.3033e-04 - root_mean_squared_error: 0.0270\n",
      "Epoch 6/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 7.0816e-04 - root_mean_squared_error: 0.0266\n",
      "Epoch 7/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 7.1172e-04 - root_mean_squared_error: 0.0267\n",
      "Epoch 8/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 6.9817e-04 - root_mean_squared_error: 0.0264\n",
      "Epoch 9/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 7.0502e-04 - root_mean_squared_error: 0.0265\n",
      "Epoch 10/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 6.6642e-04 - root_mean_squared_error: 0.0258\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "25 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - loss: 0.0011 - root_mean_squared_error: 0.0331\n",
      "Epoch 2/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 7.1255e-04 - root_mean_squared_error: 0.0267\n",
      "Epoch 3/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - loss: 6.6025e-04 - root_mean_squared_error: 0.0257\n",
      "Epoch 4/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.8442e-04 - root_mean_squared_error: 0.0262\n",
      "Epoch 5/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - loss: 6.5586e-04 - root_mean_squared_error: 0.0256\n",
      "Epoch 6/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.7847e-04 - root_mean_squared_error: 0.0260\n",
      "Epoch 7/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 6.3480e-04 - root_mean_squared_error: 0.0252\n",
      "Epoch 8/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.4980e-04 - root_mean_squared_error: 0.0255\n",
      "Epoch 9/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 6.6340e-04 - root_mean_squared_error: 0.0258\n",
      "Epoch 10/10\n",
      "\u001b[1m96/96\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.4925e-04 - root_mean_squared_error: 0.0255\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "24 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 46ms/step - loss: 0.0014 - root_mean_squared_error: 0.0369\n",
      "Epoch 2/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 7.0201e-04 - root_mean_squared_error: 0.0265\n",
      "Epoch 3/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 6.9728e-04 - root_mean_squared_error: 0.0264\n",
      "Epoch 4/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 6.7850e-04 - root_mean_squared_error: 0.0260\n",
      "Epoch 5/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - loss: 6.6989e-04 - root_mean_squared_error: 0.0259\n",
      "Epoch 6/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 6.7821e-04 - root_mean_squared_error: 0.0260\n",
      "Epoch 7/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.3531e-04 - root_mean_squared_error: 0.0252\n",
      "Epoch 8/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.3725e-04 - root_mean_squared_error: 0.0252\n",
      "Epoch 9/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.3710e-04 - root_mean_squared_error: 0.0252\n",
      "Epoch 10/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 6.4904e-04 - root_mean_squared_error: 0.0255\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "23 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - loss: 0.0011 - root_mean_squared_error: 0.0328\n",
      "Epoch 2/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 8.4868e-04 - root_mean_squared_error: 0.0291\n",
      "Epoch 3/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 7.8284e-04 - root_mean_squared_error: 0.0280\n",
      "Epoch 4/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 7.2496e-04 - root_mean_squared_error: 0.0269\n",
      "Epoch 5/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 7.2206e-04 - root_mean_squared_error: 0.0269\n",
      "Epoch 6/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 6.9315e-04 - root_mean_squared_error: 0.0263\n",
      "Epoch 7/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 6.5765e-04 - root_mean_squared_error: 0.0256\n",
      "Epoch 8/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 6.6906e-04 - root_mean_squared_error: 0.0259\n",
      "Epoch 9/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - loss: 6.3410e-04 - root_mean_squared_error: 0.0252\n",
      "Epoch 10/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - loss: 6.2354e-04 - root_mean_squared_error: 0.0250\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "22 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - loss: 8.9128e-04 - root_mean_squared_error: 0.0298\n",
      "Epoch 2/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 7.5552e-04 - root_mean_squared_error: 0.0275\n",
      "Epoch 3/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 7.2739e-04 - root_mean_squared_error: 0.0270\n",
      "Epoch 4/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 7.5494e-04 - root_mean_squared_error: 0.0275\n",
      "Epoch 5/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.9644e-04 - root_mean_squared_error: 0.0264\n",
      "Epoch 6/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 7.0928e-04 - root_mean_squared_error: 0.0266\n",
      "Epoch 7/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 6.4528e-04 - root_mean_squared_error: 0.0254\n",
      "Epoch 8/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.3313e-04 - root_mean_squared_error: 0.0252\n",
      "Epoch 9/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.5172e-04 - root_mean_squared_error: 0.0255\n",
      "Epoch 10/10\n",
      "\u001b[1m98/98\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.5310e-04 - root_mean_squared_error: 0.0256\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "21 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 38ms/step - loss: 9.0415e-04 - root_mean_squared_error: 0.0300\n",
      "Epoch 2/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 7.1719e-04 - root_mean_squared_error: 0.0268\n",
      "Epoch 3/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.6086e-04 - root_mean_squared_error: 0.0257\n",
      "Epoch 4/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 6.6361e-04 - root_mean_squared_error: 0.0258\n",
      "Epoch 5/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.6581e-04 - root_mean_squared_error: 0.0258\n",
      "Epoch 6/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 6.2906e-04 - root_mean_squared_error: 0.0251\n",
      "Epoch 7/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.2704e-04 - root_mean_squared_error: 0.0250\n",
      "Epoch 8/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.1419e-04 - root_mean_squared_error: 0.0248\n",
      "Epoch 9/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 6.0054e-04 - root_mean_squared_error: 0.0245\n",
      "Epoch 10/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 6.1220e-04 - root_mean_squared_error: 0.0247\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "20 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - loss: 0.0013 - root_mean_squared_error: 0.0359\n",
      "Epoch 2/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 7.8370e-04 - root_mean_squared_error: 0.0280\n",
      "Epoch 3/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 7.3068e-04 - root_mean_squared_error: 0.0270\n",
      "Epoch 4/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 7.1412e-04 - root_mean_squared_error: 0.0267\n",
      "Epoch 5/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 6.6873e-04 - root_mean_squared_error: 0.0259\n",
      "Epoch 6/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 6.7821e-04 - root_mean_squared_error: 0.0260\n",
      "Epoch 7/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 6.6520e-04 - root_mean_squared_error: 0.0258\n",
      "Epoch 8/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 6.2496e-04 - root_mean_squared_error: 0.0250\n",
      "Epoch 9/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 6.3929e-04 - root_mean_squared_error: 0.0253\n",
      "Epoch 10/10\n",
      "\u001b[1m99/99\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 6.5040e-04 - root_mean_squared_error: 0.0255\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "19 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2022 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adagrad', 'drop_out': [0.2, 0.1], 'activation': ['linear', 'tanh'], 'lr': 0.06210952184736157}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 39ms/step - loss: 0.0047 - root_mean_squared_error: 0.0657\n",
      "Epoch 2/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 7.8220e-04 - root_mean_squared_error: 0.0280\n",
      "Epoch 3/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 7.4196e-04 - root_mean_squared_error: 0.0272\n",
      "Epoch 4/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 6.8466e-04 - root_mean_squared_error: 0.0262\n",
      "Epoch 5/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 6.8313e-04 - root_mean_squared_error: 0.0261\n",
      "Epoch 6/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 6.7085e-04 - root_mean_squared_error: 0.0259\n",
      "Epoch 7/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 6.7488e-04 - root_mean_squared_error: 0.0260\n",
      "Epoch 8/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 6.3298e-04 - root_mean_squared_error: 0.0252\n",
      "Epoch 9/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 6.3185e-04 - root_mean_squared_error: 0.0251\n",
      "Epoch 10/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 6.4793e-04 - root_mean_squared_error: 0.0255\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2022 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "RF Done\n",
      "PCA Done\n",
      "18 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 139ms/step - loss: 0.0119 - root_mean_squared_error: 0.1001\n",
      "Epoch 2/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 138ms/step - loss: 5.5516e-04 - root_mean_squared_error: 0.0236\n",
      "Epoch 3/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 138ms/step - loss: 5.0855e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 4/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 139ms/step - loss: 5.1951e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 5/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 141ms/step - loss: 5.3800e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 6/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 5.0061e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 7/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 4.9555e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 8/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 4.8408e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 9/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 141ms/step - loss: 4.8442e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 10/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 141ms/step - loss: 4.7139e-04 - root_mean_squared_error: 0.0217\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "17 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 139ms/step - loss: 0.0072 - root_mean_squared_error: 0.0761\n",
      "Epoch 2/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 142ms/step - loss: 5.2773e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 3/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 4.6431e-04 - root_mean_squared_error: 0.0215\n",
      "Epoch 4/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 5.1684e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 5/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 4.8882e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 6/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 141ms/step - loss: 4.4692e-04 - root_mean_squared_error: 0.0211\n",
      "Epoch 7/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 4.3517e-04 - root_mean_squared_error: 0.0208\n",
      "Epoch 8/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 4.6498e-04 - root_mean_squared_error: 0.0216\n",
      "Epoch 9/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 139ms/step - loss: 4.2820e-04 - root_mean_squared_error: 0.0207\n",
      "Epoch 10/10\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 4.3004e-04 - root_mean_squared_error: 0.0207\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "16 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 140ms/step - loss: 0.0146 - root_mean_squared_error: 0.1097\n",
      "Epoch 2/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 141ms/step - loss: 5.7691e-04 - root_mean_squared_error: 0.0240\n",
      "Epoch 3/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 5.5523e-04 - root_mean_squared_error: 0.0235\n",
      "Epoch 4/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 141ms/step - loss: 5.3537e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 5/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 141ms/step - loss: 5.2300e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 6/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 5.1973e-04 - root_mean_squared_error: 0.0228\n",
      "Epoch 7/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 141ms/step - loss: 4.9432e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 8/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 139ms/step - loss: 4.7559e-04 - root_mean_squared_error: 0.0218\n",
      "Epoch 9/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.5914e-04 - root_mean_squared_error: 0.0214\n",
      "Epoch 10/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 4.6553e-04 - root_mean_squared_error: 0.0216\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "15 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 159ms/step - loss: 0.0093 - root_mean_squared_error: 0.0855\n",
      "Epoch 2/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 159ms/step - loss: 5.3046e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 3/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - loss: 5.3546e-04 - root_mean_squared_error: 0.0231\n",
      "Epoch 4/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 159ms/step - loss: 5.2653e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 5/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - loss: 5.1471e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 6/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - loss: 4.8286e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 7/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - loss: 4.9576e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 8/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - loss: 5.0043e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 9/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - loss: 4.5120e-04 - root_mean_squared_error: 0.0212\n",
      "Epoch 10/10\n",
      "\u001b[1m102/102\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 162ms/step - loss: 4.6830e-04 - root_mean_squared_error: 0.0216\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "14 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 141ms/step - loss: 0.0080 - root_mean_squared_error: 0.0804\n",
      "Epoch 2/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 5.0402e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 3/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 4.7788e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 4/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 4.7776e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 5/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 4.9687e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 6/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 4.5277e-04 - root_mean_squared_error: 0.0213\n",
      "Epoch 7/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.2882e-04 - root_mean_squared_error: 0.0207\n",
      "Epoch 8/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - loss: 4.5953e-04 - root_mean_squared_error: 0.0214\n",
      "Epoch 9/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 4.2292e-04 - root_mean_squared_error: 0.0206\n",
      "Epoch 10/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 145ms/step - loss: 3.9099e-04 - root_mean_squared_error: 0.0198\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "13 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 160ms/step - loss: 0.0210 - root_mean_squared_error: 0.1306\n",
      "Epoch 2/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 160ms/step - loss: 5.4269e-04 - root_mean_squared_error: 0.0233\n",
      "Epoch 3/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 154ms/step - loss: 5.0105e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 4/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 5.1220e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 5/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 5.0828e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 6/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 5.0421e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 7/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.8480e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 8/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.7956e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 9/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.5692e-04 - root_mean_squared_error: 0.0214\n",
      "Epoch 10/10\n",
      "\u001b[1m103/103\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.5657e-04 - root_mean_squared_error: 0.0214\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "12 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 153ms/step - loss: 0.0118 - root_mean_squared_error: 0.0961\n",
      "Epoch 2/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 5.3778e-04 - root_mean_squared_error: 0.0232\n",
      "Epoch 3/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 5.2312e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 4/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 4.7307e-04 - root_mean_squared_error: 0.0217\n",
      "Epoch 5/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 4.9464e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 6/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 4.9623e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 7/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 5.1038e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 8/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 5.0080e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 9/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 4.8368e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 10/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 4.7839e-04 - root_mean_squared_error: 0.0219\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "11 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 141ms/step - loss: 0.0065 - root_mean_squared_error: 0.0732\n",
      "Epoch 2/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.8536e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 3/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.7460e-04 - root_mean_squared_error: 0.0218\n",
      "Epoch 4/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 4.7910e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 5/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.5665e-04 - root_mean_squared_error: 0.0214\n",
      "Epoch 6/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 4.7826e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 7/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.5600e-04 - root_mean_squared_error: 0.0213\n",
      "Epoch 8/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.2676e-04 - root_mean_squared_error: 0.0206\n",
      "Epoch 9/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 4.6611e-04 - root_mean_squared_error: 0.0216\n",
      "Epoch 10/10\n",
      "\u001b[1m104/104\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 145ms/step - loss: 4.4897e-04 - root_mean_squared_error: 0.0212\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "10 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 141ms/step - loss: 0.0101 - root_mean_squared_error: 0.0931\n",
      "Epoch 2/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 159ms/step - loss: 5.2461e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 3/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.9804e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 4/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 4.8045e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 5/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 144ms/step - loss: 5.2464e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 6/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.8186e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 7/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 5.0741e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 8/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.8130e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 9/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.6863e-04 - root_mean_squared_error: 0.0216\n",
      "Epoch 10/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 141ms/step - loss: 4.8151e-04 - root_mean_squared_error: 0.0219\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "9 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 140ms/step - loss: 0.0073 - root_mean_squared_error: 0.0762\n",
      "Epoch 2/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 139ms/step - loss: 5.4634e-04 - root_mean_squared_error: 0.0234\n",
      "Epoch 3/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.8865e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 4/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.7803e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 5/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.4368e-04 - root_mean_squared_error: 0.0211\n",
      "Epoch 6/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.5285e-04 - root_mean_squared_error: 0.0213\n",
      "Epoch 7/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.6585e-04 - root_mean_squared_error: 0.0216\n",
      "Epoch 8/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.1202e-04 - root_mean_squared_error: 0.0203\n",
      "Epoch 9/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 142ms/step - loss: 4.0330e-04 - root_mean_squared_error: 0.0201\n",
      "Epoch 10/10\n",
      "\u001b[1m105/105\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 156ms/step - loss: 3.8597e-04 - root_mean_squared_error: 0.0196\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 278ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "8 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 140ms/step - loss: 0.0110 - root_mean_squared_error: 0.0962\n",
      "Epoch 2/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 137ms/step - loss: 4.8674e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 3/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 137ms/step - loss: 5.0633e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 4/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 137ms/step - loss: 4.7292e-04 - root_mean_squared_error: 0.0217\n",
      "Epoch 5/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.5818e-04 - root_mean_squared_error: 0.0214\n",
      "Epoch 6/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 139ms/step - loss: 4.5847e-04 - root_mean_squared_error: 0.0214\n",
      "Epoch 7/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.6793e-04 - root_mean_squared_error: 0.0216\n",
      "Epoch 8/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 139ms/step - loss: 4.2216e-04 - root_mean_squared_error: 0.0205\n",
      "Epoch 9/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 139ms/step - loss: 4.4129e-04 - root_mean_squared_error: 0.0210\n",
      "Epoch 10/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 139ms/step - loss: 4.0775e-04 - root_mean_squared_error: 0.0202\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "7 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2023 with hyperparameters of {'num_layers': 4, 'units': [128, 32, 32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1, 0.1, 0.2], 'activation': ['tanh', 'tanh', 'linear', 'linear'], 'lr': 0.019671144721368838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 139ms/step - loss: 0.0104 - root_mean_squared_error: 0.0922\n",
      "Epoch 2/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.9044e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 3/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 5.0453e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 4/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 5.1552e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 5/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 5.0276e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 6/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 143ms/step - loss: 4.7667e-04 - root_mean_squared_error: 0.0218\n",
      "Epoch 7/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 158ms/step - loss: 4.8313e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 8/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 167ms/step - loss: 4.5075e-04 - root_mean_squared_error: 0.0212\n",
      "Epoch 9/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 151ms/step - loss: 4.5747e-04 - root_mean_squared_error: 0.0214\n",
      "Epoch 10/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - loss: 4.2830e-04 - root_mean_squared_error: 0.0207\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 298ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 63, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2023 with hyperparameters of {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "RF Done\n",
      "PCA Done\n",
      "6 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2024 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.004409350423921994}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - loss: 0.0010 - root_mean_squared_error: 0.0311\n",
      "Epoch 2/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 5.2834e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 3/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 4.8390e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 4/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 5.3116e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 5/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 5.2657e-04 - root_mean_squared_error: 0.0229\n",
      "Epoch 6/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 5.1020e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 7/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 4.9709e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 8/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 4.9383e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 9/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 4.9743e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 10/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 4.9368e-04 - root_mean_squared_error: 0.0222\n",
      "\u001b[1m4/4\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n",
      "RF Done\n",
      "PCA Done\n",
      "5 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2024 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.004409350423921994}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - loss: 9.3224e-04 - root_mean_squared_error: 0.0300\n",
      "Epoch 2/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 4.9680e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 3/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 5.0874e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 4/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 4.9961e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 5/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - loss: 5.0867e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 6/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 5.0503e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 7/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 4.8068e-04 - root_mean_squared_error: 0.0219\n",
      "Epoch 8/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 4.8775e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 9/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 4.6609e-04 - root_mean_squared_error: 0.0216\n",
      "Epoch 10/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 4.7987e-04 - root_mean_squared_error: 0.0219\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 259ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n",
      "RF Done\n",
      "PCA Done\n",
      "4 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2024 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.004409350423921994}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 48ms/step - loss: 9.5213e-04 - root_mean_squared_error: 0.0303\n",
      "Epoch 2/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 5.1206e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 3/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - loss: 5.0250e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 4/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - loss: 4.9898e-04 - root_mean_squared_error: 0.0223\n",
      "Epoch 5/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - loss: 4.7743e-04 - root_mean_squared_error: 0.0218\n",
      "Epoch 6/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 4.5726e-04 - root_mean_squared_error: 0.0214\n",
      "Epoch 7/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - loss: 4.7525e-04 - root_mean_squared_error: 0.0218\n",
      "Epoch 8/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 64ms/step - loss: 5.0095e-04 - root_mean_squared_error: 0.0224\n",
      "Epoch 9/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - loss: 4.8366e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 10/10\n",
      "\u001b[1m106/106\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 57ms/step - loss: 4.7539e-04 - root_mean_squared_error: 0.0218\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 555ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n",
      "RF Done\n",
      "PCA Done\n",
      "3 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2024 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.004409350423921994}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 8.0976e-04 - root_mean_squared_error: 0.0281\n",
      "Epoch 2/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 5.0498e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 3/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 4.9334e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 4/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 4.9416e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 5/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 4.7351e-04 - root_mean_squared_error: 0.0218\n",
      "Epoch 6/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 4.6817e-04 - root_mean_squared_error: 0.0216\n",
      "Epoch 7/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 4.9133e-04 - root_mean_squared_error: 0.0222\n",
      "Epoch 8/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - loss: 4.7587e-04 - root_mean_squared_error: 0.0218\n",
      "Epoch 9/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 48ms/step - loss: 4.8324e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 10/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 4.6725e-04 - root_mean_squared_error: 0.0216\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 547ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n",
      "RF Done\n",
      "PCA Done\n",
      "2 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2024 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.004409350423921994}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 9.5939e-04 - root_mean_squared_error: 0.0303\n",
      "Epoch 2/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - loss: 5.1212e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 3/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 5.1341e-04 - root_mean_squared_error: 0.0227\n",
      "Epoch 4/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 4.8900e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 5/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 4.7150e-04 - root_mean_squared_error: 0.0217\n",
      "Epoch 6/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 5.0419e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 7/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 4.8524e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 8/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 4.7175e-04 - root_mean_squared_error: 0.0217\n",
      "Epoch 9/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 4.8235e-04 - root_mean_squared_error: 0.0220\n",
      "Epoch 10/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 4.9685e-04 - root_mean_squared_error: 0.0223\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 529ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n",
      "RF Done\n",
      "PCA Done\n",
      "1 dates remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Done\n",
      "Ridge Done\n",
      "test_year is 2024 with hyperparameters of {'num_layers': 2, 'units': [32, 32], 'optimizer': 'Adam', 'drop_out': [0.2, 0.1], 'activation': ['tanh', 'tanh'], 'lr': 0.004409350423921994}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - loss: 8.4190e-04 - root_mean_squared_error: 0.0285\n",
      "Epoch 2/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 5.2905e-04 - root_mean_squared_error: 0.0230\n",
      "Epoch 3/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 5.1069e-04 - root_mean_squared_error: 0.0226\n",
      "Epoch 4/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - loss: 5.0424e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 5/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 5.0475e-04 - root_mean_squared_error: 0.0225\n",
      "Epoch 6/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 4.8658e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 7/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - loss: 4.8928e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 8/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 4.8918e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 9/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 4.8923e-04 - root_mean_squared_error: 0.0221\n",
      "Epoch 10/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 4.7142e-04 - root_mean_squared_error: 0.0217\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 522ms/step\n",
      "LSTM Done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best XGBoost Parameters: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n",
      "XGBoost done\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best LGBM Parameters: {'subsample': 0.9, 'num_leaves': 31, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM done\n",
      "test_year is 2024 with hyperparameters of {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n",
      "RF Done\n",
      "PCA Done\n",
      "0 dates remaining\n"
     ]
    }
   ],
   "source": [
    "# Run to train the models\n",
    "y_tech = overall_function(dataset=df_tech, outcome = \"rolling_alpha_5f\", \n",
    "                          lstm_hyperparams_dict_tech=lstm_hyperparams_dict_tech, rf_hyperparams_dict_tech=rf_hyperparams_dict_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tech.to_csv(\"tech_funds_forecast_tuned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Hyperparameter Tuning (Healthcare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date']) # converting to date format\n",
    "df = df.sort_values(by='date')\n",
    "df_factor = df.drop(columns=['mkt_return','mth_return','rf']) # remove irrelevant variables\n",
    "\n",
    "# Creating Lagged and Stepped Datasets\n",
    "X_dataset, y_dataset = create_stepped_dataset(create_lagged_dataset(df_factor, lag=1,target_var='rolling_alpha_5f', id = 'crsp_fundno'),step=1,target_var='rolling_alpha_5f', id = 'crsp_fundno_L1')\n",
    "# return(X_dataset)\n",
    "X_dataset = X_dataset.drop(columns=['crsp_fundno_L1'], errors='ignore')\n",
    "\n",
    "list_of_dates = pd.to_datetime(X_dataset['date_L1'])\n",
    "percentile_70 = list_of_dates.quantile(0.7) # 70-30 split\n",
    "\n",
    "train_end = list_of_dates.loc[(list_of_dates - percentile_70).abs().idxmin()]\n",
    "df_end = list_of_dates.max()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 300, 500, 1000],  # Number of trees\n",
    "    'max_depth': [10, 20, 30, None],  # Depth of trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples for split\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2', None]  # Number of features considered per split\n",
    "}\n",
    "\n",
    "def rf_tuner(X_train, y_train, rf_param_grid):\n",
    "    # Initialize RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(random_state=40, n_jobs=-1)\n",
    "\n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    rf_random_search = RandomizedSearchCV(\n",
    "        estimator=rf_model,\n",
    "        param_distributions=rf_param_grid,\n",
    "        n_iter=20,  # Number of parameter settings tested\n",
    "        cv=tscv,  # TimeSeriesSplit\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    rf_random_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    # Best hyperparameters\n",
    "    best_rf_params = rf_random_search.best_params_\n",
    "    print(f'Best RF Parameters: {best_rf_params}')\n",
    "    return(best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "Train End is 2019-12-31 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Test Date is 2020-01-31 00:00:00, year is 2020, which means it is being trained until the end of 2019. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "Train End is 2020-02-28 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-03-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-04-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-05-29 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-06-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-07-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-08-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-09-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-10-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-11-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-12-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Test Date is 2021-01-29 00:00:00, year is 2021, which means it is being trained until the end of 2020. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "Train End is 2021-02-26 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-03-31 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-04-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-05-28 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-06-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-07-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-08-31 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-09-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-10-29 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-11-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-12-31 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Test Date is 2022-01-31 00:00:00, year is 2022, which means it is being trained until the end of 2021. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "Train End is 2022-02-28 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-03-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-04-29 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-05-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-06-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-07-29 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-08-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-09-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-10-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-11-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-12-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Test Date is 2023-01-31 00:00:00, year is 2023, which means it is being trained until the end of 2022. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "Train End is 2023-02-28 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-03-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-04-28 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-05-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-06-30 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-07-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-08-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-09-29 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-10-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-11-30 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-12-29 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Test Date is 2024-01-31 00:00:00, year is 2024, which means it is being trained until the end of 2023. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "Train End is 2024-02-29 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-03-28 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-04-30 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-05-31 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-06-28 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-07-31 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-08-30 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-09-30 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is NaT, year is nan, which is same as current year 2024. Repeating until following year is obtained\n"
     ]
    }
   ],
   "source": [
    "hyperparam_lst = []\n",
    "\n",
    "# 2019\n",
    "test_date = generate_next_date(list_of_dates, train_end)\n",
    "X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "initial_best_rf_params = rf_tuner(X_train, y_train, rf_param_grid)\n",
    "hyperparam_lst.append(initial_best_rf_params)\n",
    "current_year = 2019\n",
    "\n",
    "while pd.notna(test_date):\n",
    "    test_date = generate_next_date(list_of_dates, train_end)\n",
    "    if test_date.year != (current_year + 1): # Still same year: skip and repeat until the year is the next year\n",
    "        train_end = test_date\n",
    "        print(f\"Train End is {train_end}, year is {train_end.year}, which is same as current year {current_year}. Repeating until following year is obtained\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Test Date is {test_date}, year is {test_date.year}, which means it is being trained until the end of {current_year}. Conducting Tuning\")\n",
    "        # Process data for modeling\n",
    "        X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "        best_rf_params = rf_tuner(X_train, y_train, rf_param_grid)\n",
    "        hyperparam_lst.append(best_rf_params)\n",
    "        train_end = test_date # after tuning, expand the training set\n",
    "        current_year = current_year + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Hyperparam Tuning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, Adagrad, Nadam\n",
    "def build_lstm_model(hp):\n",
    "    lstm_model = Sequential()\n",
    "    # Select the number of LSTM layers (2, 3, or 4)\n",
    "    num_layers = hp.Choice('num_layers', [2, 3, 4])\n",
    "    # Choose optimizer\n",
    "    optimizer_name = hp.Choice('optimizer', ['Adam', 'Adagrad'])\n",
    "    optimizers = {\n",
    "        \"Adam\": Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='LOG')),\n",
    "        \"Adagrad\": Adagrad(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='LOG')),\n",
    "        \"Nadam\": Nadam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='LOG'))\n",
    "    }\n",
    "    # Layer 1\n",
    "    first_layer_units = hp.Int('lstm_units_1', min_value=32, max_value=128, step=32)\n",
    "    lstm_model.add(LSTM(\n",
    "        units=first_layer_units,\n",
    "        return_sequences=True,\n",
    "        input_shape=(X_train_lstm.shape[1], 1),\n",
    "        activation=hp.Choice(f'activation_1', ['tanh', 'sigmoid', 'linear'])\n",
    "    ))\n",
    "    lstm_model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    \n",
    "    # Add remaining layers with non-increasing units\n",
    "    previous_units = first_layer_units  # Track previous layer units\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        available_units = [u for u in [32, 64, 96, 128] if u <= previous_units]  # Only allow non-increasing sizes\n",
    "        current_units = hp.Choice(f'lstm_units_{i+1}', available_units)\n",
    "\n",
    "        lstm_model.add(LSTM(\n",
    "            units=current_units,\n",
    "            return_sequences=True if i < num_layers - 1 else False,  \n",
    "            activation=hp.Choice(f'activation_{i+1}', ['tanh', 'sigmoid', 'linear'])\n",
    "        ))\n",
    "        lstm_model.add(Dropout(hp.Float(f'dropout_{i+1}', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "        previous_units = current_units  # Update tracking variable\n",
    "    \n",
    "    # Output Layer\n",
    "    lstm_model.add(Dense(units=1))\n",
    "    \n",
    "    # Compile the model\n",
    "    lstm_model.compile(\n",
    "        optimizer=optimizers[optimizer_name],\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return lstm_model\n",
    "\n",
    "def tune_lstm(X_train_lstm, y_train_lstm, year, save_path=\"best_lstm_model\"):\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_lstm_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=50,\n",
    "        executions_per_trial=1,\n",
    "        directory=f'lstm_tuning_{year}_tech',\n",
    "        project_name='fund_forecasting'\n",
    "    )\n",
    "\n",
    "    # Perform the search with batch_size as a hyperparameter to tune\n",
    "    tuner.search(\n",
    "        X_train_lstm, y_train_lstm,\n",
    "        epochs=10,\n",
    "        validation_split=0.2,\n",
    "        batch_size=512,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "    )\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    \n",
    "    # Print the best hyperparameters\n",
    "    print(\"Best Hyperparameters:\", best_hps.values)\n",
    "\n",
    "    return best_hps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Hyperparameter Tuning for 2019 - 2024 --> Yearly Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from lstm_tuning_2019_tech\\fund_forecasting\\tuner0.json\n",
      "Best Hyperparameters: {'num_layers': 2, 'optimizer': 'Adam', 'learning_rate': 0.04130929884537057, 'lstm_units_1': 32, 'activation_1': 'linear', 'dropout_1': 0.1, 'lstm_units_2': 32, 'activation_2': 'tanh', 'dropout_2': 0.1, 'lstm_units_3': 32, 'activation_3': 'tanh', 'dropout_3': 0.1, 'lstm_units_4': 32, 'activation_4': 'sigmoid', 'dropout_4': 0.2}\n",
      "Train End is 2019-02-28 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-03-29 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-04-30 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-05-31 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-06-28 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-07-31 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-08-30 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-09-30 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-10-31 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-11-29 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-12-31 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Test Date is 2020-01-31 00:00:00, year is 2020, which means it is being trained until the end of 2019. Conducting Tuning\n",
      "Reloading Tuner from lstm_tuning_2020_tech\\fund_forecasting\\tuner0.json\n",
      "Best Hyperparameters: {'num_layers': 2, 'optimizer': 'Adam', 'learning_rate': 0.0017545326289340611, 'lstm_units_1': 96, 'activation_1': 'tanh', 'dropout_1': 0.1, 'lstm_units_2': 32, 'activation_2': 'tanh', 'dropout_2': 0.1, 'lstm_units_3': 32, 'activation_3': 'sigmoid', 'dropout_3': 0.2, 'lstm_units_4': 32, 'activation_4': 'sigmoid', 'dropout_4': 0.1}\n",
      "Train End is 2020-02-28 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-03-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-04-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-05-29 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-06-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-07-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-08-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-09-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-10-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-11-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-12-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Test Date is 2021-01-29 00:00:00, year is 2021, which means it is being trained until the end of 2020. Conducting Tuning\n",
      "Reloading Tuner from lstm_tuning_2021_tech\\fund_forecasting\\tuner0.json\n",
      "Best Hyperparameters: {'num_layers': 2, 'optimizer': 'Adam', 'learning_rate': 0.008514542675036612, 'lstm_units_1': 96, 'activation_1': 'linear', 'dropout_1': 0.1, 'lstm_units_2': 32, 'activation_2': 'tanh', 'dropout_2': 0.2, 'lstm_units_3': 32, 'activation_3': 'linear', 'dropout_3': 0.2, 'lstm_units_4': 32, 'activation_4': 'tanh', 'dropout_4': 0.1}\n",
      "Train End is 2021-02-26 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-03-31 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-04-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-05-28 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-06-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-07-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-08-31 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-09-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-10-29 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-11-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-12-31 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Test Date is 2022-01-31 00:00:00, year is 2022, which means it is being trained until the end of 2021. Conducting Tuning\n",
      "Reloading Tuner from lstm_tuning_2022_tech\\fund_forecasting\\tuner0.json\n",
      "Best Hyperparameters: {'num_layers': 2, 'optimizer': 'Adagrad', 'learning_rate': 0.06210952184736157, 'lstm_units_1': 32, 'activation_1': 'linear', 'dropout_1': 0.2, 'lstm_units_2': 32, 'activation_2': 'tanh', 'dropout_2': 0.1, 'lstm_units_3': 32, 'activation_3': 'linear', 'dropout_3': 0.2, 'lstm_units_4': 32, 'activation_4': 'linear', 'dropout_4': 0.1}\n",
      "Train End is 2022-02-28 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-03-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-04-29 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-05-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-06-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-07-29 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-08-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-09-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-10-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-11-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-12-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Test Date is 2023-01-31 00:00:00, year is 2023, which means it is being trained until the end of 2022. Conducting Tuning\n",
      "Reloading Tuner from lstm_tuning_2023_tech\\fund_forecasting\\tuner0.json\n",
      "Best Hyperparameters: {'num_layers': 4, 'optimizer': 'Adam', 'learning_rate': 0.019671144721368838, 'lstm_units_1': 128, 'activation_1': 'tanh', 'dropout_1': 0.2, 'lstm_units_2': 32, 'activation_2': 'tanh', 'dropout_2': 0.1, 'lstm_units_3': 32, 'activation_3': 'linear', 'dropout_3': 0.1, 'lstm_units_4': 32, 'activation_4': 'linear', 'dropout_4': 0.2}\n",
      "Train End is 2023-02-28 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-03-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-04-28 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-05-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-06-30 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-07-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-08-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-09-29 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-10-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-11-30 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-12-29 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Test Date is 2024-01-31 00:00:00, year is 2024, which means it is being trained until the end of 2023. Conducting Tuning\n",
      "Reloading Tuner from lstm_tuning_2024_tech\\fund_forecasting\\tuner0.json\n",
      "Best Hyperparameters: {'num_layers': 2, 'optimizer': 'Adam', 'learning_rate': 0.004409350423921994, 'lstm_units_1': 32, 'activation_1': 'tanh', 'dropout_1': 0.2, 'lstm_units_2': 32, 'activation_2': 'tanh', 'dropout_2': 0.1, 'lstm_units_3': 32, 'activation_3': 'sigmoid', 'dropout_3': 0.1, 'lstm_units_4': 32, 'activation_4': 'sigmoid', 'dropout_4': 0.1}\n",
      "Train End is 2024-02-29 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-03-28 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-04-30 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-05-31 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-06-28 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is NaT, year is nan, which is same as current year 2024. Repeating until following year is obtained\n"
     ]
    }
   ],
   "source": [
    "df_tech['date'] = pd.to_datetime(df_tech['date']) # converting to date format\n",
    "df_tech = df_tech.sort_values(by='date')\n",
    "df_factor = df_tech.drop(columns=['mkt_return','mth_return','rf','rolling_sharpe', 'rolling_alpha_3f', 'rolling_alpha_4f'])\n",
    "\n",
    "# Creating Lagged and Stepped Datasets\n",
    "X_dataset, y_dataset = create_stepped_dataset(create_lagged_dataset(df_factor, lag=1,target_var='rolling_alpha_5f', id = 'crsp_fundno'),step=1,target_var='rolling_alpha_5f', id = 'crsp_fundno_L1')\n",
    "# return(X_dataset)\n",
    "X_dataset = X_dataset.drop(columns=['crsp_fundno_L1'], errors='ignore')\n",
    "\n",
    "list_of_dates = pd.to_datetime(X_dataset['date_L1'])\n",
    "percentile_70 = list_of_dates.quantile(0.7) # 70-30 split\n",
    "\n",
    "train_end = list_of_dates.loc[(list_of_dates - percentile_70).abs().idxmin()]\n",
    "df_end = list_of_dates.max()\n",
    "\n",
    "hyperparam_lst = []\n",
    "\n",
    "# 2019\n",
    "current_year = 2019\n",
    "test_date = generate_next_date(list_of_dates, train_end)\n",
    "X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "X_train_lstm = np.array(X_train).astype(np.float32)\n",
    "X_test_lstm = np.array(X_test).astype(np.float32)\n",
    "y_train_lstm = np.array(y_train).astype(np.float32)\n",
    "best_model_params = tune_lstm(X_train_lstm, y_train_lstm, year = current_year)\n",
    "hyperparam_lst.append(best_model_params)\n",
    "# current_year = 2020\n",
    "\n",
    "while pd.notna(test_date):\n",
    "    test_date = generate_next_date(list_of_dates, train_end)\n",
    "    if test_date.year != (current_year + 1): # Still same year: skip and repeat until the year is the next year\n",
    "        train_end = test_date\n",
    "        print(f\"Train End is {train_end}, year is {train_end.year}, which is same as current year {current_year}. Repeating until following year is obtained\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Test Date is {test_date}, year is {test_date.year}, which means it is being trained until the end of {current_year}. Conducting Tuning\")\n",
    "        # Process data for modeling\n",
    "        X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "        X_train_lstm = np.array(X_train).astype(np.float32)\n",
    "        X_test_lstm = np.array(X_test).astype(np.float32)\n",
    "        y_train_lstm = np.array(y_train).astype(np.float32)\n",
    "        best_model_params = tune_lstm(X_train_lstm, y_train_lstm, year = current_year+1)\n",
    "        hyperparam_lst.append(best_model_params)\n",
    "        train_end = test_date # after tuning, expand the training set\n",
    "        current_year = current_year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "tscv = TimeSeriesSplit(n_splits = 5)\n",
    "\n",
    "df_tech['date'] = pd.to_datetime(df_tech['date']) # converting to date format\n",
    "df_tech = df_tech.sort_values(by='date')\n",
    "df_factor = df_tech.drop(columns=['mkt_return','mth_return','rf','rolling_sharpe', 'rolling_alpha_3f', 'rolling_alpha_4f'])\n",
    "\n",
    "# Creating Lagged and Stepped Datasets\n",
    "X_dataset, y_dataset = create_stepped_dataset(create_lagged_dataset(df_factor, lag=1,target_var='rolling_alpha_5f', id = 'crsp_fundno'),step=1,target_var='rolling_alpha_5f', id = 'crsp_fundno_L1')\n",
    "# return(X_dataset)\n",
    "X_dataset = X_dataset.drop(columns=['crsp_fundno_L1'], errors='ignore')\n",
    "\n",
    "list_of_dates = pd.to_datetime(X_dataset['date_L1'])\n",
    "percentile_70 = list_of_dates.quantile(0.7) # 70-30 split\n",
    "\n",
    "train_end = list_of_dates.loc[(list_of_dates - percentile_70).abs().idxmin()]\n",
    "df_end = list_of_dates.max()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 300, 500, 1000],  # Number of trees\n",
    "    'max_depth': [10, 20, 30, None],  # Depth of trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples for split\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum samples per leaf\n",
    "    'max_features': ['sqrt', 'log2', None]  # Number of features considered per split\n",
    "}\n",
    "\n",
    "def rf_tuner(X_train, y_train, rf_param_grid):\n",
    "    # Initialize RandomForestRegressor\n",
    "    rf_model = RandomForestRegressor(random_state=40, n_jobs=-1)\n",
    "\n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    rf_random_search = RandomizedSearchCV(\n",
    "        estimator=rf_model,\n",
    "        param_distributions=rf_param_grid,\n",
    "        n_iter=20,  # Number of parameter settings tested\n",
    "        cv=tscv,  # TimeSeriesSplit\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    rf_random_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    # Best hyperparameters\n",
    "    best_rf_params = rf_random_search.best_params_\n",
    "    print(f'Best RF Parameters: {best_rf_params}')\n",
    "    return(best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "Train End is 2019-02-28 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-03-29 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-04-30 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-05-31 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-06-28 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-07-31 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-08-30 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-09-30 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-10-31 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-11-29 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Train End is 2019-12-31 00:00:00, year is 2019, which is same as current year 2019. Repeating until following year is obtained\n",
      "Test Date is 2020-01-31 00:00:00, year is 2020, which means it is being trained until the end of 2019. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "Train End is 2020-02-28 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-03-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-04-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-05-29 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-06-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-07-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-08-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-09-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-10-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-11-30 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Train End is 2020-12-31 00:00:00, year is 2020, which is same as current year 2020. Repeating until following year is obtained\n",
      "Test Date is 2021-01-29 00:00:00, year is 2021, which means it is being trained until the end of 2020. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "Train End is 2021-02-26 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-03-31 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-04-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-05-28 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-06-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-07-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-08-31 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-09-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-10-29 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-11-30 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Train End is 2021-12-31 00:00:00, year is 2021, which is same as current year 2021. Repeating until following year is obtained\n",
      "Test Date is 2022-01-31 00:00:00, year is 2022, which means it is being trained until the end of 2021. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': None}\n",
      "Train End is 2022-02-28 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-03-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-04-29 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-05-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-06-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-07-29 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-08-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-09-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-10-31 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-11-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Train End is 2022-12-30 00:00:00, year is 2022, which is same as current year 2022. Repeating until following year is obtained\n",
      "Test Date is 2023-01-31 00:00:00, year is 2023, which means it is being trained until the end of 2022. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 20}\n",
      "Train End is 2023-02-28 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-03-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-04-28 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-05-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-06-30 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-07-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-08-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-09-29 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-10-31 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-11-30 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Train End is 2023-12-29 00:00:00, year is 2023, which is same as current year 2023. Repeating until following year is obtained\n",
      "Test Date is 2024-01-31 00:00:00, year is 2024, which means it is being trained until the end of 2023. Conducting Tuning\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best RF Parameters: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}\n",
      "Train End is 2024-02-29 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-03-28 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-04-30 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-05-31 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is 2024-06-28 00:00:00, year is 2024, which is same as current year 2024. Repeating until following year is obtained\n",
      "Train End is NaT, year is nan, which is same as current year 2024. Repeating until following year is obtained\n"
     ]
    }
   ],
   "source": [
    "hyperparam_lst_rf = []\n",
    "\n",
    "# 2019\n",
    "test_date = generate_next_date(list_of_dates, train_end)\n",
    "X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "initial_best_rf_params = rf_tuner(X_train, y_train, rf_param_grid)\n",
    "hyperparam_lst_rf.append(initial_best_rf_params)\n",
    "current_year = 2019\n",
    "\n",
    "while pd.notna(test_date):\n",
    "    test_date = generate_next_date(list_of_dates, train_end)\n",
    "    if test_date.year != (current_year + 1): # Still same year: skip and repeat until the year is the next year\n",
    "        train_end = test_date\n",
    "        print(f\"Train End is {train_end}, year is {train_end.year}, which is same as current year {current_year}. Repeating until following year is obtained\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Test Date is {test_date}, year is {test_date.year}, which means it is being trained until the end of {current_year}. Conducting Tuning\")\n",
    "        # Process data for modeling\n",
    "        X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "        best_rf_params = rf_tuner(X_train, y_train, rf_param_grid)\n",
    "        hyperparam_lst_rf.append(best_rf_params)\n",
    "        train_end = test_date # after tuning, expand the training set\n",
    "        current_year = current_year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_estimators': 300,\n",
       "  'min_samples_split': 5,\n",
       "  'min_samples_leaf': 4,\n",
       "  'max_features': 'log2',\n",
       "  'max_depth': 20},\n",
       " {'n_estimators': 300,\n",
       "  'min_samples_split': 5,\n",
       "  'min_samples_leaf': 4,\n",
       "  'max_features': 'log2',\n",
       "  'max_depth': 20},\n",
       " {'n_estimators': 300,\n",
       "  'min_samples_split': 5,\n",
       "  'min_samples_leaf': 4,\n",
       "  'max_features': 'log2',\n",
       "  'max_depth': 20},\n",
       " {'n_estimators': 100,\n",
       "  'min_samples_split': 10,\n",
       "  'min_samples_leaf': 4,\n",
       "  'max_features': 'log2',\n",
       "  'max_depth': None},\n",
       " {'n_estimators': 300,\n",
       "  'min_samples_split': 5,\n",
       "  'min_samples_leaf': 4,\n",
       "  'max_features': 'log2',\n",
       "  'max_depth': 20},\n",
       " {'n_estimators': 100,\n",
       "  'min_samples_split': 5,\n",
       "  'min_samples_leaf': 4,\n",
       "  'max_features': 'log2',\n",
       "  'max_depth': 30}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam_lst_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 2018 model until 2019-12-31 00:00:00. Conducting hyperparameter tuning...\n",
      "No pretrained model found. Running full hyperparameter tuning...\n",
      "Reloading Tuner from lstm_tuning_2019\\fund_forecasting\\tuner0.json\n",
      "Best Hyperparameters for 2019: {'num_layers': 4, 'optimizer': 'Nadam', 'learning_rate': 0.007390757774868635, 'lstm_units_1': 32, 'activation_1': 'linear', 'dropout_1': 0.1, 'lstm_units_2': 32, 'activation_2': 'tanh', 'dropout_2': 0.2, 'lstm_units_3': 32, 'activation_3': 'sigmoid', 'dropout_3': 0.1, 'lstm_units_4': 32, 'activation_4': 'tanh', 'dropout_4': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 116ms/step - loss: 0.0245 - root_mean_squared_error: 0.1340 - val_loss: 2.4114e-04 - val_root_mean_squared_error: 0.0155\n",
      "Epoch 2/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 106ms/step - loss: 5.5111e-04 - root_mean_squared_error: 0.0235 - val_loss: 2.4324e-04 - val_root_mean_squared_error: 0.0156\n",
      "Epoch 3/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 104ms/step - loss: 5.2553e-04 - root_mean_squared_error: 0.0229 - val_loss: 2.5315e-04 - val_root_mean_squared_error: 0.0159\n",
      "Epoch 4/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 104ms/step - loss: 4.9586e-04 - root_mean_squared_error: 0.0223 - val_loss: 2.3901e-04 - val_root_mean_squared_error: 0.0155\n",
      "Epoch 5/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 106ms/step - loss: 4.8268e-04 - root_mean_squared_error: 0.0220 - val_loss: 2.5097e-04 - val_root_mean_squared_error: 0.0158\n",
      "Epoch 6/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 108ms/step - loss: 5.0658e-04 - root_mean_squared_error: 0.0225 - val_loss: 2.2655e-04 - val_root_mean_squared_error: 0.0151\n",
      "Epoch 7/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 104ms/step - loss: 4.5960e-04 - root_mean_squared_error: 0.0214 - val_loss: 2.4906e-04 - val_root_mean_squared_error: 0.0158\n",
      "Epoch 8/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 106ms/step - loss: 4.7802e-04 - root_mean_squared_error: 0.0218 - val_loss: 2.2146e-04 - val_root_mean_squared_error: 0.0149\n",
      "Epoch 9/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 105ms/step - loss: 5.6695e-04 - root_mean_squared_error: 0.0238 - val_loss: 2.4936e-04 - val_root_mean_squared_error: 0.0158\n",
      "Epoch 10/10\n",
      "\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 108ms/step - loss: 5.2677e-04 - root_mean_squared_error: 0.0229 - val_loss: 2.5237e-04 - val_root_mean_squared_error: 0.0159\n",
      "Model saved as best_lstm_model_2019.keras\n",
      "\n",
      "Training 2019 model until 2020-01-31 00:00:00. Conducting hyperparameter tuning...\n",
      "Loading pretrained model from best_lstm_model_2019.keras...\n",
      "Epoch 1/10\n",
      "\u001b[1m202/202\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 112ms/step - loss: 5.2310e-04 - root_mean_squared_error: 0.0229 - val_loss: 2.7666e-04 - val_root_mean_squared_error: 0.0166\n",
      "Epoch 2/10\n",
      "\u001b[1m202/202\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 103ms/step - loss: 5.2269e-04 - root_mean_squared_error: 0.0229 - val_loss: 2.6076e-04 - val_root_mean_squared_error: 0.0161\n",
      "Epoch 3/10\n",
      "\u001b[1m202/202\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 105ms/step - loss: 5.1848e-04 - root_mean_squared_error: 0.0228 - val_loss: 2.6949e-04 - val_root_mean_squared_error: 0.0164\n",
      "Epoch 4/10\n",
      "\u001b[1m202/202\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 106ms/step - loss: 5.4422e-04 - root_mean_squared_error: 0.0233 - val_loss: 2.6641e-04 - val_root_mean_squared_error: 0.0163\n",
      "Epoch 5/10\n",
      "\u001b[1m202/202\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 104ms/step - loss: 5.3240e-04 - root_mean_squared_error: 0.0231 - val_loss: 2.6567e-04 - val_root_mean_squared_error: 0.0163\n",
      "Model saved as best_lstm_model_2020.keras\n",
      "Train End: 2020-02-28 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-03-31 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-04-30 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-05-29 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-06-30 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-07-31 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-08-31 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-09-30 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-10-30 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-11-30 00:00:00, same year 2020. Repeating until next year.\n",
      "Train End: 2020-12-31 00:00:00, same year 2020. Repeating until next year.\n",
      "\n",
      "Training 2020 model until 2021-01-29 00:00:00. Conducting hyperparameter tuning...\n",
      "Loading pretrained model from best_lstm_model_2020.keras...\n",
      "Epoch 1/10\n",
      "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 114ms/step - loss: 5.0425e-04 - root_mean_squared_error: 0.0224 - val_loss: 6.8372e-04 - val_root_mean_squared_error: 0.0261\n",
      "Epoch 2/10\n",
      "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 107ms/step - loss: 5.0528e-04 - root_mean_squared_error: 0.0225 - val_loss: 6.8932e-04 - val_root_mean_squared_error: 0.0263\n",
      "Epoch 3/10\n",
      "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 104ms/step - loss: 4.6933e-04 - root_mean_squared_error: 0.0217 - val_loss: 6.7702e-04 - val_root_mean_squared_error: 0.0260\n",
      "Epoch 4/10\n",
      "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - loss: 5.3020e-04 - root_mean_squared_error: 0.0230 - val_loss: 6.7715e-04 - val_root_mean_squared_error: 0.0260\n",
      "Epoch 5/10\n",
      "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - loss: 4.9680e-04 - root_mean_squared_error: 0.0223 - val_loss: 6.9503e-04 - val_root_mean_squared_error: 0.0264\n",
      "Epoch 6/10\n",
      "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - loss: 4.9413e-04 - root_mean_squared_error: 0.0222 - val_loss: 6.7671e-04 - val_root_mean_squared_error: 0.0260\n",
      "Epoch 7/10\n",
      "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - loss: 4.9056e-04 - root_mean_squared_error: 0.0221 - val_loss: 6.7878e-04 - val_root_mean_squared_error: 0.0261\n",
      "Epoch 8/10\n",
      "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - loss: 4.8834e-04 - root_mean_squared_error: 0.0221 - val_loss: 6.7759e-04 - val_root_mean_squared_error: 0.0260\n",
      "Epoch 9/10\n",
      "\u001b[1m222/222\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 102ms/step - loss: 5.0143e-04 - root_mean_squared_error: 0.0224 - val_loss: 6.7755e-04 - val_root_mean_squared_error: 0.0260\n",
      "Model saved as best_lstm_model_2021.keras\n",
      "Train End: 2021-02-26 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-03-31 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-04-30 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-05-28 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-06-30 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-07-30 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-08-31 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-09-30 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-10-29 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-11-30 00:00:00, same year 2021. Repeating until next year.\n",
      "Train End: 2021-12-31 00:00:00, same year 2021. Repeating until next year.\n",
      "\n",
      "Training 2021 model until 2022-01-31 00:00:00. Conducting hyperparameter tuning...\n",
      "Loading pretrained model from best_lstm_model_2021.keras...\n",
      "Epoch 1/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 110ms/step - loss: 4.8067e-04 - root_mean_squared_error: 0.0219 - val_loss: 7.1946e-04 - val_root_mean_squared_error: 0.0268\n",
      "Epoch 2/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 105ms/step - loss: 4.7160e-04 - root_mean_squared_error: 0.0217 - val_loss: 7.0986e-04 - val_root_mean_squared_error: 0.0266\n",
      "Epoch 3/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 104ms/step - loss: 4.7219e-04 - root_mean_squared_error: 0.0217 - val_loss: 7.0807e-04 - val_root_mean_squared_error: 0.0266\n",
      "Epoch 4/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 102ms/step - loss: 5.0759e-04 - root_mean_squared_error: 0.0225 - val_loss: 7.2769e-04 - val_root_mean_squared_error: 0.0270\n",
      "Epoch 5/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step - loss: 4.6791e-04 - root_mean_squared_error: 0.0216 - val_loss: 7.1015e-04 - val_root_mean_squared_error: 0.0266\n",
      "Epoch 6/10\n",
      "\u001b[1m242/242\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 112ms/step - loss: 4.6056e-04 - root_mean_squared_error: 0.0214 - val_loss: 7.0814e-04 - val_root_mean_squared_error: 0.0266\n",
      "Model saved as best_lstm_model_2022.keras\n",
      "Train End: 2022-02-28 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-03-31 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-04-29 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-05-31 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-06-30 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-07-29 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-08-31 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-09-30 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-10-31 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-11-30 00:00:00, same year 2022. Repeating until next year.\n",
      "Train End: 2022-12-30 00:00:00, same year 2022. Repeating until next year.\n",
      "\n",
      "Training 2022 model until 2023-01-31 00:00:00. Conducting hyperparameter tuning...\n",
      "Loading pretrained model from best_lstm_model_2022.keras...\n",
      "Epoch 1/10\n",
      "\u001b[1m262/262\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 111ms/step - loss: 5.4844e-04 - root_mean_squared_error: 0.0234 - val_loss: 4.1272e-04 - val_root_mean_squared_error: 0.0203\n",
      "Epoch 2/10\n",
      "\u001b[1m262/262\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 105ms/step - loss: 5.4234e-04 - root_mean_squared_error: 0.0233 - val_loss: 4.3696e-04 - val_root_mean_squared_error: 0.0209\n",
      "Epoch 3/10\n",
      "\u001b[1m262/262\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 107ms/step - loss: 5.3744e-04 - root_mean_squared_error: 0.0231 - val_loss: 4.4861e-04 - val_root_mean_squared_error: 0.0212\n",
      "Epoch 4/10\n",
      "\u001b[1m262/262\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 107ms/step - loss: 5.0939e-04 - root_mean_squared_error: 0.0226 - val_loss: 4.3690e-04 - val_root_mean_squared_error: 0.0209\n",
      "Model saved as best_lstm_model_2023.keras\n",
      "Train End: 2023-02-28 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-03-31 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-04-28 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-05-31 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-06-30 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-07-31 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-08-31 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-09-29 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-10-31 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-11-30 00:00:00, same year 2023. Repeating until next year.\n",
      "Train End: 2023-12-29 00:00:00, same year 2023. Repeating until next year.\n",
      "\n",
      "Training 2023 model until 2024-01-31 00:00:00. Conducting hyperparameter tuning...\n",
      "Loading pretrained model from best_lstm_model_2023.keras...\n",
      "Epoch 1/10\n",
      "\u001b[1m279/279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 109ms/step - loss: 4.9640e-04 - root_mean_squared_error: 0.0223 - val_loss: 3.1703e-04 - val_root_mean_squared_error: 0.0178\n",
      "Epoch 2/10\n",
      "\u001b[1m279/279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 104ms/step - loss: 5.0145e-04 - root_mean_squared_error: 0.0224 - val_loss: 3.1971e-04 - val_root_mean_squared_error: 0.0179\n",
      "Epoch 3/10\n",
      "\u001b[1m279/279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 106ms/step - loss: 5.5883e-04 - root_mean_squared_error: 0.0236 - val_loss: 3.2602e-04 - val_root_mean_squared_error: 0.0181\n",
      "Epoch 4/10\n",
      "\u001b[1m279/279\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 104ms/step - loss: 4.7720e-04 - root_mean_squared_error: 0.0218 - val_loss: 3.7910e-04 - val_root_mean_squared_error: 0.0195\n",
      "Model saved as best_lstm_model_2024.keras\n",
      "Train End: 2024-02-29 00:00:00, same year 2024. Repeating until next year.\n",
      "Train End: 2024-03-28 00:00:00, same year 2024. Repeating until next year.\n",
      "Train End: 2024-04-30 00:00:00, same year 2024. Repeating until next year.\n",
      "Train End: 2024-05-31 00:00:00, same year 2024. Repeating until next year.\n",
      "Train End: 2024-06-28 00:00:00, same year 2024. Repeating until next year.\n",
      "Train End: 2024-07-31 00:00:00, same year 2024. Repeating until next year.\n",
      "Train End: 2024-08-30 00:00:00, same year 2024. Repeating until next year.\n",
      "Train End: 2024-09-30 00:00:00, same year 2024. Repeating until next year.\n",
      "Train End: NaT, same year 2024. Repeating until next year.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, Nadam\n",
    "import keras_tuner as kt\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "# Convert date column to datetime and sort\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(by='date')\n",
    "\n",
    "# Remove irrelevant variables\n",
    "df_factor = df.drop(columns=['mkt_return', 'mth_return', 'rf'], errors='ignore')\n",
    "\n",
    "# Create lagged and stepped datasets\n",
    "X_dataset, y_dataset = create_stepped_dataset(\n",
    "    create_lagged_dataset(df_factor, lag=1, target_var='rolling_alpha_5f', id='crsp_fundno'),\n",
    "    step=1,\n",
    "    target_var='rolling_alpha_5f',\n",
    "    id='crsp_fundno_L1'\n",
    ")\n",
    "\n",
    "# Drop fund ID column (if present)\n",
    "X_dataset = X_dataset.drop(columns=['crsp_fundno_L1'], errors='ignore')\n",
    "\n",
    "# Define train-test split based on date\n",
    "list_of_dates = pd.to_datetime(X_dataset['date_L1'])\n",
    "percentile_70 = list_of_dates.quantile(0.7)\n",
    "\n",
    "train_end = list_of_dates.loc[(list_of_dates - percentile_70).abs().idxmin()]\n",
    "df_end = list_of_dates.max()\n",
    "\n",
    "hyperparam_lst = []\n",
    "\n",
    "# Start from 2019\n",
    "current_year = 2018\n",
    "test_date = generate_next_date(list_of_dates, train_end)\n",
    "pretrained_model = None\n",
    "\n",
    "# ---- Function to Build LSTM Model ----\n",
    "def build_lstm_model(hp):\n",
    "    lstm_model = Sequential()\n",
    "    \n",
    "    num_layers = hp.Choice('num_layers', [2, 3, 4])  # Choose number of LSTM layers\n",
    "    optimizer_name = hp.Choice('optimizer', ['Adam', 'Adagrad', 'Nadam'])  # Choose optimizer\n",
    "\n",
    "    optimizers = {\n",
    "        \"Adam\": Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='LOG')),\n",
    "        \"Adagrad\": Adagrad(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='LOG')),\n",
    "        \"Nadam\": Nadam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='LOG'))\n",
    "    }\n",
    "    \n",
    "    first_layer_units = hp.Int('lstm_units_1', min_value=32, max_value=128, step=32)\n",
    "    lstm_model.add(LSTM(\n",
    "        units=first_layer_units,\n",
    "        return_sequences=True,\n",
    "        input_shape=(X_train_lstm.shape[1], 1),\n",
    "        activation=hp.Choice('activation_1', ['tanh', 'sigmoid', 'linear'])\n",
    "    ))\n",
    "    lstm_model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "\n",
    "    previous_units = first_layer_units\n",
    "\n",
    "    for i in range(1, num_layers):\n",
    "        available_units = [u for u in [32, 64, 96, 128] if u <= previous_units]\n",
    "        current_units = hp.Choice(f'lstm_units_{i+1}', available_units)\n",
    "\n",
    "        lstm_model.add(LSTM(\n",
    "            units=current_units,\n",
    "            return_sequences=True if i < num_layers - 1 else False,\n",
    "            activation=hp.Choice(f'activation_{i+1}', ['tanh', 'sigmoid', 'linear'])\n",
    "        ))\n",
    "        lstm_model.add(Dropout(hp.Float(f'dropout_{i+1}', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "        previous_units = current_units  \n",
    "\n",
    "    # Output layer\n",
    "    lstm_model.add(Dense(units=1))\n",
    "\n",
    "    # Compile model\n",
    "    lstm_model.compile(\n",
    "        optimizer=optimizers[optimizer_name],\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return lstm_model\n",
    "\n",
    "# ---- Function to Tune or Fine-Tune LSTM ----\n",
    "def tune_lstm(X_train_lstm, y_train_lstm, year, pretrained_model=None, save_path=\"best_lstm_model\"):\n",
    "    \"\"\"Trains or fine-tunes an LSTM model with hyperparameter tuning.\"\"\"\n",
    "    \n",
    "    model_path = f\"{save_path}_{year}.keras\"\n",
    "    \n",
    "    if pretrained_model and os.path.exists(pretrained_model):\n",
    "        print(f\"Loading pretrained model from {pretrained_model}...\")\n",
    "        lstm_model = load_model(pretrained_model)\n",
    "    else:\n",
    "        print(\"No pretrained model found. Running full hyperparameter tuning...\")\n",
    "        tuner = kt.BayesianOptimization(\n",
    "            build_lstm_model,\n",
    "            objective='val_loss',\n",
    "            max_trials=50,\n",
    "            executions_per_trial=1,\n",
    "            directory=f'lstm_tuning_{year}',\n",
    "            project_name='fund_forecasting'\n",
    "        )\n",
    "\n",
    "        tuner.search(\n",
    "            X_train_lstm, y_train_lstm,\n",
    "            epochs=50,\n",
    "            validation_split=0.2,\n",
    "            batch_size=128,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "        )\n",
    "\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        print(f\"Best Hyperparameters for {year}: {best_hps.values}\")\n",
    "        lstm_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # Train (or fine-tune) the model\n",
    "    lstm_model.fit(\n",
    "        X_train_lstm, y_train_lstm,\n",
    "        epochs=20,\n",
    "        validation_split=0.2,\n",
    "        batch_size=128,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "    )\n",
    "\n",
    "    # Save model for next year's training\n",
    "    lstm_model.save(model_path)\n",
    "    print(f\"Model saved as {model_path}\")\n",
    "    \n",
    "    return model_path  # Return saved model path\n",
    "\n",
    "# ---- Loop Through Years for Transfer Learning ----\n",
    "while pd.notna(test_date):\n",
    "    test_date = generate_next_date(list_of_dates, train_end)\n",
    "    \n",
    "    if test_date.year != (current_year + 1):\n",
    "        train_end = test_date\n",
    "        print(f\"Train End: {train_end}, same year {current_year}. Repeating until next year.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nTraining {current_year} model until {test_date}. Conducting hyperparameter tuning...\")\n",
    "    \n",
    "    # Prepare data for training\n",
    "    X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "    X_train_lstm = np.array(X_train).astype(np.float32)\n",
    "    X_test_lstm = np.array(X_test).astype(np.float32)\n",
    "    y_train_lstm = np.array(y_train).astype(np.float32)\n",
    "\n",
    "    # Train or fine-tune model\n",
    "    pretrained_model = tune_lstm(X_train_lstm, y_train_lstm, year=current_year+1, pretrained_model=pretrained_model)\n",
    "    hyperparam_lst.append(pretrained_model)\n",
    "\n",
    "    # Update year and training end date\n",
    "    train_end = test_date\n",
    "    current_year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, Nadam\n",
    "import keras_tuner as kt\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "# Convert date column to datetime and sort\n",
    "df_tech['date'] = pd.to_datetime(df_tech['date'])\n",
    "df_tech = df_tech.sort_values(by='date')\n",
    "\n",
    "# Remove irrelevant variables\n",
    "df_factor = df_tech.drop(columns=['mkt_return', 'mth_return', 'rf'], errors='ignore')\n",
    "\n",
    "# Create lagged and stepped datasets\n",
    "X_dataset, y_dataset = create_stepped_dataset(\n",
    "    create_lagged_dataset(df_factor, lag=1, target_var='rolling_alpha_5f', id='crsp_fundno'),\n",
    "    step=1,\n",
    "    target_var='rolling_alpha_5f',\n",
    "    id='crsp_fundno_L1'\n",
    ")\n",
    "\n",
    "# Drop fund ID column (if present)\n",
    "X_dataset = X_dataset.drop(columns=['crsp_fundno_L1'], errors='ignore')\n",
    "\n",
    "# Define train-test split based on date\n",
    "list_of_dates = pd.to_datetime(X_dataset['date_L1'])\n",
    "percentile_70 = list_of_dates.quantile(0.7)\n",
    "\n",
    "train_end = list_of_dates.loc[(list_of_dates - percentile_70).abs().idxmin()]\n",
    "df_end = list_of_dates.max()\n",
    "\n",
    "hyperparam_lst = []\n",
    "\n",
    "# Start from 2019\n",
    "current_year = 2018\n",
    "test_date = generate_next_date(list_of_dates, train_end)\n",
    "pretrained_model = None\n",
    "\n",
    "# ---- Function to Build LSTM Model ----\n",
    "def build_lstm_model(hp):\n",
    "    lstm_model = Sequential()\n",
    "    \n",
    "    num_layers = hp.Choice('num_layers', [2, 3, 4])  # Choose number of LSTM layers\n",
    "    optimizer_name = hp.Choice('optimizer', ['Adam', 'Adagrad', 'Nadam'])  # Choose optimizer\n",
    "\n",
    "    optimizers = {\n",
    "        \"Adam\": Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='LOG')),\n",
    "        \"Adagrad\": Adagrad(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='LOG')),\n",
    "        \"Nadam\": Nadam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-1, sampling='LOG'))\n",
    "    }\n",
    "    \n",
    "    first_layer_units = hp.Int('lstm_units_1', min_value=32, max_value=128, step=32)\n",
    "    lstm_model.add(LSTM(\n",
    "        units=first_layer_units,\n",
    "        return_sequences=True,\n",
    "        input_shape=(X_train_lstm.shape[1], 1),\n",
    "        activation=hp.Choice('activation_1', ['tanh', 'sigmoid', 'linear'])\n",
    "    ))\n",
    "    lstm_model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "\n",
    "    previous_units = first_layer_units\n",
    "\n",
    "    for i in range(1, num_layers):\n",
    "        available_units = [u for u in [32, 64, 96, 128] if u <= previous_units]\n",
    "        current_units = hp.Choice(f'lstm_units_{i+1}', available_units)\n",
    "\n",
    "        lstm_model.add(LSTM(\n",
    "            units=current_units,\n",
    "            return_sequences=True if i < num_layers - 1 else False,\n",
    "            activation=hp.Choice(f'activation_{i+1}', ['tanh', 'sigmoid', 'linear'])\n",
    "        ))\n",
    "        lstm_model.add(Dropout(hp.Float(f'dropout_{i+1}', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "        previous_units = current_units  \n",
    "\n",
    "    # Output layer\n",
    "    lstm_model.add(Dense(units=1))\n",
    "\n",
    "    # Compile model\n",
    "    lstm_model.compile(\n",
    "        optimizer=optimizers[optimizer_name],\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return lstm_model\n",
    "\n",
    "# ---- Function to Tune or Fine-Tune LSTM ----\n",
    "def tune_lstm(X_train_lstm, y_train_lstm, year, pretrained_model=None, save_path=\"best_lstm_model\"):\n",
    "    \"\"\"Trains or fine-tunes an LSTM model with hyperparameter tuning.\"\"\"\n",
    "    \n",
    "    model_path = f\"{save_path}_{year}_tech.keras\"\n",
    "    \n",
    "    if pretrained_model and os.path.exists(pretrained_model):\n",
    "        print(f\"Loading pretrained model from {pretrained_model}...\")\n",
    "        lstm_model = load_model(pretrained_model)\n",
    "    else:\n",
    "        print(\"No pretrained model found. Running full hyperparameter tuning...\")\n",
    "        tuner = kt.BayesianOptimization(\n",
    "            build_lstm_model,\n",
    "            objective='val_loss',\n",
    "            max_trials=50,\n",
    "            executions_per_trial=1,\n",
    "            directory=f'lstm_tuning_{year}',\n",
    "            project_name='fund_forecasting'\n",
    "        )\n",
    "\n",
    "        tuner.search(\n",
    "            X_train_lstm, y_train_lstm,\n",
    "            epochs=50,\n",
    "            validation_split=0.2,\n",
    "            batch_size=128,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "        )\n",
    "\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        print(f\"Best Hyperparameters for {year}: {best_hps.values}\")\n",
    "        lstm_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # Train (or fine-tune) the model\n",
    "    lstm_model.fit(\n",
    "        X_train_lstm, y_train_lstm,\n",
    "        epochs=20,\n",
    "        validation_split=0.2,\n",
    "        batch_size=128,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "    )\n",
    "\n",
    "    # Save model for next year's training\n",
    "    lstm_model.save(model_path)\n",
    "    print(f\"Model saved as {model_path}\")\n",
    "    \n",
    "    return model_path  # Return saved model path\n",
    "\n",
    "# ---- Loop Through Years for Transfer Learning ----\n",
    "while pd.notna(test_date):\n",
    "    test_date = generate_next_date(list_of_dates, train_end)\n",
    "    \n",
    "    if test_date.year != (current_year + 1):\n",
    "        train_end = test_date\n",
    "        print(f\"Train End: {train_end}, same year {current_year}. Repeating until next year.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nTraining {current_year} model until {test_date}. Conducting hyperparameter tuning...\")\n",
    "    \n",
    "    # Prepare data for training\n",
    "    X_train, X_test, y_train, y_test = process_factor_model(X_dataset, y_dataset, train_end, test_date)\n",
    "    X_train_lstm = np.array(X_train).astype(np.float32)\n",
    "    X_test_lstm = np.array(X_test).astype(np.float32)\n",
    "    y_train_lstm = np.array(y_train).astype(np.float32)\n",
    "\n",
    "    # Train or fine-tune model\n",
    "    pretrained_model = tune_lstm(X_train_lstm, y_train_lstm, year=current_year+1, pretrained_model=pretrained_model)\n",
    "    hyperparam_lst.append(pretrained_model)\n",
    "\n",
    "    # Update year and training end date\n",
    "    train_end = test_date\n",
    "    current_year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_hyperparams_dict_healthcare = dict(\n",
    "#     {'2019': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.2,0.2],\n",
    "#         'activation':['tanh','sigmoid'],\n",
    "#         'lr':0.01995625160697196\n",
    "#     }),\n",
    "#      '2020': dict({\n",
    "#         'num_layers':3,\n",
    "#         'units':[32,32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.2,0.2,0.1],\n",
    "#         'activation':['linear','sigmoid','sigmoid'],\n",
    "#         'lr':0.0011839804874350056\n",
    "#     }),\n",
    "#      '2021': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[128,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.2,0.1],\n",
    "#         'activation':['tanh','linear'],\n",
    "#         'lr':0.018389663547277172\n",
    "#     }),\n",
    "#      '2022': dict({\n",
    "#         'num_layers':4,\n",
    "#         'units':[64,32,32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.1,0.1,0.2,0.2],\n",
    "#         'activation':['linear','tanh','tanh','linear'],\n",
    "#         'lr':0.011965744369339311\n",
    "#     }),\n",
    "#      '2023': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.2,0.1],\n",
    "#         'activation':['tanh','tanh'],\n",
    "#         'lr':0.0005130193097484537\n",
    "#     }),\n",
    "#      '2024': dict({\n",
    "#         'num_layers':2,\n",
    "#         'units':[32,32],\n",
    "#         'optimizer':'Adam',\n",
    "#         'drop_out':[0.1,0.2],\n",
    "#         'activation':['linear','tanh'],\n",
    "#         'lr':0.0035163892485339547\n",
    "#     })}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
