{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crsp_fundno', 'date', 'mth_return', 'exp_ratio', 'turn_ratio',\n",
       "       'normalised_flow', 'gdp_to_debt_ratio', 'gdp_growth_rate', 'unm_rate',\n",
       "       'infl_rate', 'mktrf', 'smb', 'hml', 'rmw', 'cma', 'rf', 'umd',\n",
       "       'excess_return', 'rolling_sharpe', 'mkt_return', 'rolling_alpha_3f',\n",
       "       'rolling_alpha_4f', 'rolling_alpha_5f', 'shortrun_momentum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import bisect\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, SGDRegressor\n",
    "df = pd.read_csv(\"df_ff_factors_010225.csv\")\n",
    "df = df.drop(columns=[\"Unnamed: 0\",\"crsp_portno\"])\n",
    "df = df.sort_values(by='date')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rolling_sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43632</th>\n",
       "      <td>2.216452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43633</th>\n",
       "      <td>3.058133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748</th>\n",
       "      <td>-0.536935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43634</th>\n",
       "      <td>1.604677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13749</th>\n",
       "      <td>0.429084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>1.619481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>0.691406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>0.691738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>0.676394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>0.631615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54349 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rolling_sharpe\n",
       "43632        2.216452\n",
       "43633        3.058133\n",
       "13748       -0.536935\n",
       "43634        1.604677\n",
       "13749        0.429084\n",
       "...               ...\n",
       "3031         1.619481\n",
       "1960         0.691406\n",
       "2113         0.691738\n",
       "1842         0.676394\n",
       "1901         0.631615\n",
       "\n",
       "[54349 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Lagged Dataset\n",
    "def create_lagged_dataset(dataset, lag, target_var, id):\n",
    "    lagged_dataset = dataset.copy()\n",
    "    columns_list = list(lagged_dataset.columns)\n",
    "    data_join = {}\n",
    "    for column in columns_list:\n",
    "        if column == target_var:\n",
    "            data_join[column] = lagged_dataset[column]\n",
    "        for n in range(1,lag+1):\n",
    "            data_join[F'{column}_L{n}'] = lagged_dataset.groupby(id)[column].shift(n)\n",
    "    lagged_dataset = pd.concat(data_join.values(), axis=1, ignore_index = True)\n",
    "    lagged_dataset.columns = data_join.keys()\n",
    "    return lagged_dataset.dropna()\n",
    "\n",
    "# Generate Stepped Dataset for Training\n",
    "## Steps is the number of months ahead that we are forecasting, e.g. step=2 is 2 months ahead.\n",
    "## Note step=1 results in no change to dataset, i.e. use generated lagged variables to forecast current. \n",
    "def create_stepped_dataset(dataset, step, target_var, id):\n",
    "    \n",
    "    shifted_dataset = dataset.copy()\n",
    "    # y = shifted_dataset[[target_var]].shift(-step+1)\n",
    "    # if step != 1:\n",
    "    #     X = shifted_dataset.iloc[:-step+1, :] # remove the last few rows\n",
    "    # else:\n",
    "    #     X = shifted_dataset\n",
    "    # return X.drop(target_var, axis = 1), y.dropna()\n",
    "    shifted_dataset['shifted_target'] = shifted_dataset.groupby(id)[target_var].shift(-step + 1)\n",
    "    \n",
    "    # Drop rows where the shifted target is NaN (these occur due to the shift operation)\n",
    "    shifted_dataset = shifted_dataset.dropna(subset=['shifted_target'])\n",
    "    \n",
    "    # Separate the features (X) and the target (y)\n",
    "    X = shifted_dataset.drop(columns=[target_var, 'shifted_target'])\n",
    "    y = shifted_dataset[['shifted_target']]\n",
    "    y = y.rename(columns={'shifted_target':target_var})\n",
    "    return X, y\n",
    "\n",
    "x = create_lagged_dataset(df, lag = 1, target_var='rolling_sharpe', id = 'crsp_fundno')\n",
    "test_step = create_stepped_dataset(x, step=1, target_var='rolling_sharpe', id = 'crsp_fundno_L1')[1]\n",
    "test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.16425066],\n",
       "       [0.16425066, 1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "corr = np.corrcoef(df['rolling_alpha_5f'], df['rolling_sharpe'])\n",
    "corr # 0.204 --> can use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-Factor Model: mktrf, SMB, HML\n",
    "4-Factor Model: mktrf, SMB, HML, UMD\n",
    "5-Factor Model: mktrf, SMB, HML, RMW, CMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crsp_fundno</th>\n",
       "      <th>date</th>\n",
       "      <th>exp_ratio</th>\n",
       "      <th>turn_ratio</th>\n",
       "      <th>normalised_flow</th>\n",
       "      <th>gdp_to_debt_ratio</th>\n",
       "      <th>gdp_growth_rate</th>\n",
       "      <th>unm_rate</th>\n",
       "      <th>infl_rate</th>\n",
       "      <th>mktrf</th>\n",
       "      <th>smb</th>\n",
       "      <th>hml</th>\n",
       "      <th>umd</th>\n",
       "      <th>excess_return</th>\n",
       "      <th>rolling_sharpe</th>\n",
       "      <th>rolling_alpha_3f</th>\n",
       "      <th>rolling_alpha_4f</th>\n",
       "      <th>rolling_alpha_5f</th>\n",
       "      <th>shortrun_momentum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43631</th>\n",
       "      <td>32553.0</td>\n",
       "      <td>1993-08-31</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.009529</td>\n",
       "      <td>64.101</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>3.248937</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>0.012866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43632</th>\n",
       "      <td>32553.0</td>\n",
       "      <td>1993-09-30</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>64.101</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.90</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>2.216452</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>-0.004108</td>\n",
       "      <td>-0.009113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43633</th>\n",
       "      <td>32553.0</td>\n",
       "      <td>1993-10-29</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.030641</td>\n",
       "      <td>64.669</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>-0.0276</td>\n",
       "      <td>-0.0272</td>\n",
       "      <td>0.040962</td>\n",
       "      <td>3.058133</td>\n",
       "      <td>0.040962</td>\n",
       "      <td>0.040962</td>\n",
       "      <td>0.040962</td>\n",
       "      <td>0.000383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13747</th>\n",
       "      <td>12051.0</td>\n",
       "      <td>1993-10-29</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.070030</td>\n",
       "      <td>64.669</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>-0.0276</td>\n",
       "      <td>-0.0272</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>5.744340</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>0.034690</td>\n",
       "      <td>0.013103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748</th>\n",
       "      <td>12051.0</td>\n",
       "      <td>1993-11-30</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>64.669</td>\n",
       "      <td>3.5225</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.90</td>\n",
       "      <td>-0.0189</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>-0.0074</td>\n",
       "      <td>-0.0474</td>\n",
       "      <td>-0.075999</td>\n",
       "      <td>-0.536935</td>\n",
       "      <td>-0.075999</td>\n",
       "      <td>-0.075999</td>\n",
       "      <td>-0.075999</td>\n",
       "      <td>-0.026033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>4610.0</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.650182</td>\n",
       "      <td>120.040</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>-0.0247</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.078518</td>\n",
       "      <td>1.619481</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>-0.025985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>4330.0</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.022800</td>\n",
       "      <td>120.731</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>-0.0247</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.028784</td>\n",
       "      <td>0.691406</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>-0.024241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>4333.0</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.402964</td>\n",
       "      <td>120.731</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>-0.0247</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.028779</td>\n",
       "      <td>0.691738</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>0.010953</td>\n",
       "      <td>-0.024228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>4327.0</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.032107</td>\n",
       "      <td>120.731</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>-0.0247</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.676394</td>\n",
       "      <td>0.004506</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>0.010697</td>\n",
       "      <td>-0.024451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>4329.0</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-1.025886</td>\n",
       "      <td>120.731</td>\n",
       "      <td>2.5427</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>-0.0247</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.027821</td>\n",
       "      <td>0.631615</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>-0.025131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54718 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       crsp_fundno        date  exp_ratio  turn_ratio  normalised_flow  \\\n",
       "43631      32553.0  1993-08-31     0.0162        0.15         0.009529   \n",
       "43632      32553.0  1993-09-30     0.0162        0.15         0.014881   \n",
       "43633      32553.0  1993-10-29     0.0162        0.15         0.030641   \n",
       "13747      12051.0  1993-10-29     0.0188        0.77         0.070030   \n",
       "13748      12051.0  1993-11-30     0.0188        0.77         0.000259   \n",
       "...            ...         ...        ...         ...              ...   \n",
       "3031        4610.0  2024-06-28     0.0217        0.23        -0.650182   \n",
       "1960        4330.0  2024-07-31     0.0077        0.31        -0.022800   \n",
       "2113        4333.0  2024-07-31     0.0076        0.31        -0.402964   \n",
       "1842        4327.0  2024-07-31     0.0102        0.31        -0.032107   \n",
       "1901        4329.0  2024-07-31     0.0178        0.31        -1.025886   \n",
       "\n",
       "       gdp_to_debt_ratio  gdp_growth_rate  unm_rate  infl_rate   mktrf  \\\n",
       "43631             64.101           3.5225       6.8       2.90  0.0371   \n",
       "43632             64.101           3.5225       6.7       2.90 -0.0012   \n",
       "43633             64.669           3.5225       6.8       2.90  0.0141   \n",
       "13747             64.669           3.5225       6.8       2.90  0.0141   \n",
       "13748             64.669           3.5225       6.6       2.90 -0.0189   \n",
       "...                  ...              ...       ...        ...     ...   \n",
       "3031             120.040           2.5427       4.1       3.35  0.0070   \n",
       "1960             120.731           2.5427       4.2       3.35  0.0070   \n",
       "2113             120.731           2.5427       4.2       3.35  0.0070   \n",
       "1842             120.731           2.5427       4.2       3.35  0.0070   \n",
       "1901             120.731           2.5427       4.2       3.35  0.0070   \n",
       "\n",
       "          smb     hml     umd  excess_return  rolling_sharpe  \\\n",
       "43631  0.0010  0.0013  0.0265       0.068910        3.248937   \n",
       "43632  0.0298 -0.0031  0.0337      -0.004108        2.216452   \n",
       "43633  0.0189 -0.0276 -0.0272       0.040962        3.058133   \n",
       "13747  0.0189 -0.0276 -0.0272       0.034690        5.744340   \n",
       "13748 -0.0127 -0.0074 -0.0474      -0.075999       -0.536935   \n",
       "...       ...     ...     ...            ...             ...   \n",
       "3031  -0.0568 -0.0247  0.0508       0.078518        1.619481   \n",
       "1960  -0.0568 -0.0247  0.0508       0.028784        0.691406   \n",
       "2113  -0.0568 -0.0247  0.0508       0.028779        0.691738   \n",
       "1842  -0.0568 -0.0247  0.0508       0.028569        0.676394   \n",
       "1901  -0.0568 -0.0247  0.0508       0.027821        0.631615   \n",
       "\n",
       "       rolling_alpha_3f  rolling_alpha_4f  rolling_alpha_5f  shortrun_momentum  \n",
       "43631          0.068910          0.068910          0.068910           0.012866  \n",
       "43632         -0.004108         -0.004108         -0.004108          -0.009113  \n",
       "43633          0.040962          0.040962          0.040962           0.000383  \n",
       "13747          0.034690          0.034690          0.034690           0.013103  \n",
       "13748         -0.075999         -0.075999         -0.075999          -0.026033  \n",
       "...                 ...               ...               ...                ...  \n",
       "3031           0.001428          0.002329          0.003201          -0.025985  \n",
       "1960           0.004765          0.005643          0.010978          -0.024241  \n",
       "2113           0.004733          0.005601          0.010953          -0.024228  \n",
       "1842           0.004506          0.005384          0.010697          -0.024451  \n",
       "1901           0.003868          0.004746          0.010018          -0.025131  \n",
       "\n",
       "[54718 rows x 19 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3_factor = df.drop(columns=['mkt_return','umd','cma','rmw','mth_return','rf'])\n",
    "df_3_factor = df[['date','mth_return','exp_ratio','turn_ratio','mktrf', 'smb', 'hml']]\n",
    "df_4_factor = df.drop(columns=['mkt_return','cma','rmw','mth_return','rf'])\n",
    "df_5_factor = df.drop(columns=['mkt_return','umd','mth_return','rf'])\n",
    "df_4_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_naive(dataset, step, target_var):\n",
    "    naive = dataset.copy()\n",
    "    naive[target_var + '_naive'] = naive.groupby('crsp_fundno')[target_var].shift(step)\n",
    "    return naive\n",
    "\n",
    "# Generates next date\n",
    "def generate_next_date(list_of_dates, date):\n",
    "    return list_of_dates[list_of_dates > date].min()\n",
    "\n",
    "\n",
    "def process_factor_model(X_factor, y_factor, train_end, test_date):\n",
    "    X_train = X_factor[X_factor['date_L1'] <= train_end].drop(columns='date_L1')\n",
    "    X_test = X_factor[X_factor['date_L1'] == test_date].drop(columns='date_L1')\n",
    "\n",
    "    y_train = y_factor.loc[X_train.index]\n",
    "    y_test = y_factor.loc[X_test.index]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "# create_naive(df, step = 1, target_var='rolling_sharpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cycle\n",
    "def overall_function(dataset, outcome):\n",
    "    dataset['date'] = pd.to_datetime(dataset['date']) # converting to date format\n",
    "    dataset = dataset.sort_values(by='date')\n",
    "    # dataset = dataset.replace(np.inf, 0)\n",
    "    # dataset = dataset.replace(-np.inf, 0)\n",
    "\n",
    "    # return(dataset_naive)\n",
    "    # Factor Models\n",
    "    df_3_factor = dataset.drop(columns=['mkt_return','umd','cma','rmw','mth_return','rf','rolling_alpha_4f','rolling_alpha_5f'])\n",
    "    df_4_factor = dataset.drop(columns=['mkt_return','cma','rmw','mth_return','rf','rolling_alpha_3f','rolling_alpha_5f'])\n",
    "    df_5_factor = dataset.drop(columns=['mkt_return','umd','mth_return','rf','rolling_alpha_3f','rolling_alpha_4f'])\n",
    "    \n",
    "    # Creating Lagged and Stepped Datasets\n",
    "    X_dataset_3f, y_dataset_3f = create_stepped_dataset(create_lagged_dataset(df_3_factor, lag=1,target_var=outcome, id = 'crsp_fundno'),step=1,target_var=outcome, id = 'crsp_fundno_L1')\n",
    "    X_dataset_4f, y_dataset_4f = create_stepped_dataset(create_lagged_dataset(df_4_factor, lag=1,target_var=outcome, id = 'crsp_fundno'),step=1,target_var=outcome, id = 'crsp_fundno_L1')\n",
    "    X_dataset_5f, y_dataset_5f = create_stepped_dataset(create_lagged_dataset(df_5_factor, lag=1,target_var=outcome, id = 'crsp_fundno'),step=1,target_var=outcome, id = 'crsp_fundno_L1')\n",
    "    # return(y_dataset_3f)\n",
    "    list_of_dates = pd.to_datetime(X_dataset_3f['date_L1'])\n",
    "    percentile_70 = list_of_dates.quantile(0.7)\n",
    "    train_end = list_of_dates.loc[(list_of_dates - percentile_70).abs().idxmin()]\n",
    "    df_end = list_of_dates.max()\n",
    "    X_dataset_3f = X_dataset_3f.drop(columns='crsp_fundno_L1')\n",
    "    X_dataset_4f = X_dataset_4f.drop(columns='crsp_fundno_L1')\n",
    "    X_dataset_5f = X_dataset_5f.drop(columns='crsp_fundno_L1')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    while train_end != df_end:\n",
    "        \n",
    "        test_date = generate_next_date(list_of_dates, train_end)\n",
    "        if pd.isna(test_date):\n",
    "            break \n",
    "\n",
    "        # Process data for modeling\n",
    "        X_train_3f, X_test_3f, y_train_3f, y_test_3f = process_factor_model(X_dataset_3f, y_dataset_3f, train_end, test_date)\n",
    "        X_train_4f, X_test_4f, y_train_4f, y_test_4f = process_factor_model(X_dataset_4f, y_dataset_4f, train_end, test_date)\n",
    "        X_train_5f, X_test_5f, y_train_5f, y_test_5f = process_factor_model(X_dataset_5f, y_dataset_5f, train_end, test_date)\n",
    "\n",
    "        # Model 1: Naive Model\n",
    "        X_naive = X_dataset_3f[X_dataset_3f['date_L1'] == test_date][['rolling_sharpe_L1']]\n",
    "        \n",
    "        print('Naive Fitted')\n",
    "        \n",
    "        # Adding Naive Results\n",
    "        df_in_loop = y_test_3f.copy()\n",
    "        df_in_loop['naive'] = X_naive.values\n",
    "        \n",
    "        # Model 2: Linear Regression\n",
    "        linear = LinearRegression()\n",
    "        linear.fit(X_train_3f, y_train_3f.values.ravel())\n",
    "        linear3fpred = linear.predict(X_test_3f)\n",
    "        linear.fit(X_train_4f, y_train_4f.values.ravel())\n",
    "        linear4fpred = linear.predict(X_test_4f)\n",
    "        linear.fit(X_train_5f, y_train_5f.values.ravel())\n",
    "        linear5fpred = linear.predict(X_test_5f)\n",
    "        \n",
    "        print('Linear Regression 3-Factor Fitted')\n",
    "        print('Linear Regression 4-Factor Fitted')\n",
    "        print('Linear Regression 5-Factor Fitted')\n",
    "\n",
    "        ## adding\n",
    "        df_in_loop['3factlinear'] = linear3fpred\n",
    "        df_in_loop['4factlinear'] = linear4fpred\n",
    "        df_in_loop['5factlinear'] = linear5fpred\n",
    "\n",
    "        # # Model 3: Feedforward Neural Network\n",
    "        model = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=200, random_state=42)\n",
    "        \n",
    "        model.fit(X_train_3f, y_train_3f.values.ravel())\n",
    "        print('FFNN 3-Factor Fitted')\n",
    "        y_pred_3f = model.predict(X_test_3f)\n",
    "        \n",
    "        model.fit(X_train_4f, y_train_4f.values.ravel())\n",
    "        print('FFNN 4-Factor Fitted')\n",
    "        y_pred_4f = model.predict(X_test_4f)\n",
    "        \n",
    "        model.fit(X_train_5f, y_train_5f.values.ravel())\n",
    "        print('FFNN 5-Factor Fitted')\n",
    "        y_pred_5f = model.predict(X_test_5f)\n",
    "\n",
    "        # Adding FFNN results\n",
    "        df_in_loop['3f_ffnn'] = y_pred_3f\n",
    "        df_in_loop['4f_ffnn'] = y_pred_4f\n",
    "        df_in_loop['5f_ffnn'] = y_pred_5f\n",
    "        \n",
    "        # Model 4: Basic RNN\n",
    "        \n",
    "        # Model 5: Random Forest Regression\n",
    "        \n",
    "        # Add results into loop\n",
    "        results.append(df_in_loop)\n",
    "        train_end = test_date\n",
    "        num_remaining_dates = len(list(set(date for date in list_of_dates if date > test_date)))\n",
    "        print(f'{num_remaining_dates} dates remaining')\n",
    "    \n",
    "    combined_df = pd.concat(results, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "import torch\n",
    "import pandas as pd\n",
    "# num_funds = len(df['crsp_fundno'].unique())\n",
    "df['fund_index'], unique_funds = pd.factorize(df['crsp_fundno'])\n",
    "num_funds = len(unique_funds)  # Total unique funds\n",
    "embedding_dim = 10  # Example embedding size\n",
    "fund_embedding = torch.nn.Embedding(num_funds, embedding_dim)\n",
    "\n",
    "# Step 3: Convert fund indices to tensor\n",
    "fund_ids_tensor = torch.tensor(df['fund_index'].values, dtype=torch.long)\n",
    "embeddings = fund_embedding(fund_ids_tensor).detach().numpy()\n",
    "# print(embeddings.shape)  # (6, embedding_dim)\n",
    "# embeddings[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Epoch [10/500], Loss: 7.8674\n",
      "Epoch [20/500], Loss: 7.7749\n",
      "Epoch [30/500], Loss: 7.6941\n",
      "Epoch [40/500], Loss: 7.6587\n",
      "Epoch [50/500], Loss: 7.6298\n",
      "Epoch [60/500], Loss: 7.5833\n",
      "Epoch [70/500], Loss: 7.5200\n",
      "Epoch [80/500], Loss: 7.4572\n",
      "Epoch [90/500], Loss: 7.4180\n",
      "Epoch [100/500], Loss: 7.3925\n",
      "Epoch [110/500], Loss: 7.3706\n",
      "Epoch [120/500], Loss: 7.3518\n",
      "Epoch [130/500], Loss: 7.3343\n",
      "Epoch [140/500], Loss: 7.3171\n",
      "Epoch [150/500], Loss: 7.2991\n",
      "Epoch [160/500], Loss: 7.2798\n",
      "Epoch [170/500], Loss: 7.2588\n",
      "Epoch [180/500], Loss: 7.2347\n",
      "Epoch [190/500], Loss: 7.2073\n",
      "Epoch [200/500], Loss: 7.1768\n",
      "Epoch [210/500], Loss: 7.1438\n",
      "Epoch [220/500], Loss: 7.1085\n",
      "Epoch [230/500], Loss: 7.0761\n",
      "Epoch [240/500], Loss: 7.0374\n",
      "Epoch [250/500], Loss: 6.9969\n",
      "Epoch [260/500], Loss: 6.9584\n",
      "Epoch [270/500], Loss: 6.9182\n",
      "Epoch [280/500], Loss: 6.8812\n",
      "Epoch [290/500], Loss: 6.8476\n",
      "Epoch [300/500], Loss: 6.7923\n",
      "Epoch [310/500], Loss: 6.7521\n",
      "Epoch [320/500], Loss: 6.7350\n",
      "Epoch [330/500], Loss: 6.6817\n",
      "Epoch [340/500], Loss: 6.8570\n",
      "Epoch [350/500], Loss: 6.7285\n",
      "Epoch [360/500], Loss: 6.6490\n",
      "Epoch [370/500], Loss: 6.6041\n",
      "Epoch [380/500], Loss: 6.5666\n",
      "Epoch [390/500], Loss: 6.5367\n",
      "Epoch [400/500], Loss: 6.5097\n",
      "Epoch [410/500], Loss: 6.4825\n",
      "Epoch [420/500], Loss: 6.4559\n",
      "Epoch [430/500], Loss: 6.4264\n",
      "Epoch [440/500], Loss: 6.4091\n",
      "Epoch [450/500], Loss: 6.3890\n",
      "Epoch [460/500], Loss: 6.3583\n",
      "Epoch [470/500], Loss: 6.3196\n",
      "Epoch [480/500], Loss: 6.3049\n",
      "Epoch [490/500], Loss: 6.2735\n",
      "Epoch [500/500], Loss: 6.2500\n",
      "Best MSE: 5.6295 achieved with Epochs = 500 and Learning Rate = 0.01\n",
      "Predicted Returns: [[1.9753927 ]\n",
      " [0.74655175]\n",
      " [0.7080445 ]\n",
      " ...\n",
      " [0.8209151 ]\n",
      " [0.88903713]\n",
      " [0.86397374]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df['fund_index'], unique_funds = pd.factorize(df['crsp_fundno'])\n",
    "num_funds = len(unique_funds)  # Number of unique funds\n",
    "\n",
    "df_features = df.drop(columns=['crsp_fundno','date','rolling_sharpe','fund_index','rolling_alpha_3f','rolling_alpha_4f'])\n",
    "# Step 2: Convert Data to Tensors\n",
    "fund_ids_tensor = torch.tensor(df['fund_index'].values, dtype=torch.long)  # Fund IDs\n",
    "features_tensor = torch.tensor(df_features.values, dtype=torch.float32)  # Covariates\n",
    "targets_tensor = torch.tensor(df['rolling_sharpe'].values, dtype=torch.float32).unsqueeze(1)\n",
    "features_tensor = torch.nan_to_num(features_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "class FundNN(nn.Module):\n",
    "    def __init__(self, num_funds, embedding_dim, input_dim):\n",
    "        super(FundNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_funds, embedding_dim)  # Fund Embedding Layer\n",
    "        self.fc1 = nn.Linear(embedding_dim + input_dim, 64)  # Hidden Layer 1\n",
    "        self.fc2 = nn.Linear(64, 32)  # Hidden Layer 2\n",
    "        self.fc3 = nn.Linear(32, 1)  # Output Layer (Regression)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, fund_id, features):\n",
    "        embedded_fund = self.embedding(fund_id)  # Convert fund ID to embedding\n",
    "        x = torch.cat([embedded_fund, features], dim=1)  # Concatenate embedding with other features\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output regression prediction\n",
    "        return x\n",
    "\n",
    "embedding_dim = 10  # Fund embedding size\n",
    "input_dim = features_tensor.shape[1]  # Number of additional covariates (e.g., 2)\n",
    "print(input_dim)\n",
    "model = FundNN(num_funds, embedding_dim, input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()  # Loss function for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(fund_ids_tensor, features_tensor)  # Forward pass\n",
    "    loss = criterion(predictions, targets_tensor)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# epochs_list = [300, 400, 500]\n",
    "# lr_list = [0.001, 0.0025, 0.0075, 0.01]\n",
    "\n",
    "# # Store the results of MSE for each combination of epochs and learning rates\n",
    "# best_mse = float('inf')\n",
    "# best_epochs = None\n",
    "# best_lr = None\n",
    "\n",
    "# for num_epochs in epochs_list:\n",
    "#     for lr in lr_list:\n",
    "#         # Initialize model and optimizer\n",
    "#         model = FundNN(num_funds, embedding_dim, input_dim)  # Ensure model is re-initialized\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "#         # Training loop for the current combination of epochs and learning rate\n",
    "#         for epoch in range(num_epochs):\n",
    "#             optimizer.zero_grad()\n",
    "#             predictions = model(fund_ids_tensor, features_tensor)  # Forward pass\n",
    "#             loss = criterion(predictions, targets_tensor)  # Compute loss\n",
    "#             loss.backward()  # Backpropagation\n",
    "#             optimizer.step()  # Update weights\n",
    "            \n",
    "#             if (epoch + 1) % 10 == 0:\n",
    "#                 print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "#         # After training, calculate the MSE on the validation or test set\n",
    "#         with torch.no_grad():\n",
    "#             predictions = model(fund_ids_tensor, features_tensor)\n",
    "#             mse = mean_squared_error(targets_tensor.numpy(), predictions.numpy())  # Compute MSE\n",
    "            \n",
    "#             # Store the best MSE and corresponding hyperparameters\n",
    "#             if mse < best_mse:\n",
    "#                 best_mse = mse\n",
    "#                 best_epochs = num_epochs\n",
    "#                 best_lr = lr\n",
    "\n",
    "# After the loop, print the best combination of hyperparameters\n",
    "# print(f\"Best MSE: {best_mse:.4f} achieved with Epochs = {best_epochs} and Learning Rate = {best_lr}\")\n",
    "\n",
    "# Step 6: Make Predictions\n",
    "preds = model(fund_ids_tensor, features_tensor).detach().numpy()\n",
    "print(\"Predicted Returns:\", preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "64 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "63 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "62 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "61 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "60 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "59 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "58 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "57 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "56 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "55 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "54 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "53 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "52 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "51 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "50 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "49 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "48 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "47 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "46 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "45 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "44 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "43 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "42 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "41 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "40 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "39 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "38 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "37 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "36 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "35 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "34 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "33 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "32 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "31 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "30 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "29 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "28 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "27 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "26 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "25 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "24 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "23 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "22 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "21 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "20 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "19 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "18 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "17 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "16 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "15 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "14 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "13 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "12 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "11 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "10 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "9 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "8 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "7 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "6 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "5 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "4 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "3 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "2 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "1 dates remaining\n",
      "Naive Fitted\n",
      "Linear Regression 3-Factor Fitted\n",
      "Linear Regression 4-Factor Fitted\n",
      "Linear Regression 5-Factor Fitted\n",
      "FFNN 3-Factor Fitted\n",
      "FFNN 4-Factor Fitted\n",
      "FFNN 5-Factor Fitted\n",
      "0 dates remaining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rolling_sharpe</th>\n",
       "      <th>naive</th>\n",
       "      <th>3factlinear</th>\n",
       "      <th>4factlinear</th>\n",
       "      <th>5factlinear</th>\n",
       "      <th>3f_ffnn</th>\n",
       "      <th>4f_ffnn</th>\n",
       "      <th>5f_ffnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.075880</td>\n",
       "      <td>-0.175525</td>\n",
       "      <td>1.363931</td>\n",
       "      <td>1.547669</td>\n",
       "      <td>1.631241</td>\n",
       "      <td>-0.942977</td>\n",
       "      <td>-1.716935</td>\n",
       "      <td>0.223819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.620161</td>\n",
       "      <td>0.355801</td>\n",
       "      <td>1.524581</td>\n",
       "      <td>1.541747</td>\n",
       "      <td>1.609456</td>\n",
       "      <td>-0.063982</td>\n",
       "      <td>-0.602798</td>\n",
       "      <td>0.712604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.198237</td>\n",
       "      <td>-0.053156</td>\n",
       "      <td>1.562578</td>\n",
       "      <td>1.554960</td>\n",
       "      <td>1.605086</td>\n",
       "      <td>-0.158528</td>\n",
       "      <td>-0.915611</td>\n",
       "      <td>0.406972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.721097</td>\n",
       "      <td>-0.824169</td>\n",
       "      <td>1.208331</td>\n",
       "      <td>1.189372</td>\n",
       "      <td>1.320787</td>\n",
       "      <td>-0.803663</td>\n",
       "      <td>-1.628921</td>\n",
       "      <td>-0.216014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.541852</td>\n",
       "      <td>0.341311</td>\n",
       "      <td>1.546185</td>\n",
       "      <td>1.534102</td>\n",
       "      <td>1.635006</td>\n",
       "      <td>-0.032638</td>\n",
       "      <td>-0.623966</td>\n",
       "      <td>0.661172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200</th>\n",
       "      <td>0.622349</td>\n",
       "      <td>0.910776</td>\n",
       "      <td>0.681563</td>\n",
       "      <td>0.809343</td>\n",
       "      <td>0.730812</td>\n",
       "      <td>0.710803</td>\n",
       "      <td>0.729899</td>\n",
       "      <td>0.646106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16201</th>\n",
       "      <td>0.676394</td>\n",
       "      <td>0.622349</td>\n",
       "      <td>0.641445</td>\n",
       "      <td>0.732901</td>\n",
       "      <td>0.674258</td>\n",
       "      <td>0.676206</td>\n",
       "      <td>0.679003</td>\n",
       "      <td>0.873396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16202</th>\n",
       "      <td>0.691406</td>\n",
       "      <td>0.637319</td>\n",
       "      <td>0.653454</td>\n",
       "      <td>0.744909</td>\n",
       "      <td>0.687654</td>\n",
       "      <td>0.680181</td>\n",
       "      <td>0.698682</td>\n",
       "      <td>0.872134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16203</th>\n",
       "      <td>0.691738</td>\n",
       "      <td>0.637960</td>\n",
       "      <td>0.653550</td>\n",
       "      <td>0.744957</td>\n",
       "      <td>0.687835</td>\n",
       "      <td>0.679359</td>\n",
       "      <td>0.699114</td>\n",
       "      <td>0.872292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16204</th>\n",
       "      <td>0.631615</td>\n",
       "      <td>0.578175</td>\n",
       "      <td>0.606720</td>\n",
       "      <td>0.698240</td>\n",
       "      <td>0.634989</td>\n",
       "      <td>0.661866</td>\n",
       "      <td>0.619422</td>\n",
       "      <td>0.874116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16205 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rolling_sharpe     naive  3factlinear  4factlinear  5factlinear  \\\n",
       "0            0.075880 -0.175525     1.363931     1.547669     1.631241   \n",
       "1            0.620161  0.355801     1.524581     1.541747     1.609456   \n",
       "2           -0.198237 -0.053156     1.562578     1.554960     1.605086   \n",
       "3           -0.721097 -0.824169     1.208331     1.189372     1.320787   \n",
       "4            0.541852  0.341311     1.546185     1.534102     1.635006   \n",
       "...               ...       ...          ...          ...          ...   \n",
       "16200        0.622349  0.910776     0.681563     0.809343     0.730812   \n",
       "16201        0.676394  0.622349     0.641445     0.732901     0.674258   \n",
       "16202        0.691406  0.637319     0.653454     0.744909     0.687654   \n",
       "16203        0.691738  0.637960     0.653550     0.744957     0.687835   \n",
       "16204        0.631615  0.578175     0.606720     0.698240     0.634989   \n",
       "\n",
       "        3f_ffnn   4f_ffnn   5f_ffnn  \n",
       "0     -0.942977 -1.716935  0.223819  \n",
       "1     -0.063982 -0.602798  0.712604  \n",
       "2     -0.158528 -0.915611  0.406972  \n",
       "3     -0.803663 -1.628921 -0.216014  \n",
       "4     -0.032638 -0.623966  0.661172  \n",
       "...         ...       ...       ...  \n",
       "16200  0.710803  0.729899  0.646106  \n",
       "16201  0.676206  0.679003  0.873396  \n",
       "16202  0.680181  0.698682  0.872134  \n",
       "16203  0.679359  0.699114  0.872292  \n",
       "16204  0.661866  0.619422  0.874116  \n",
       "\n",
       "[16205 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = overall_function(dataset = df, outcome = 'rolling_sharpe')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rolling_sharpe</th>\n",
       "      <th>naive</th>\n",
       "      <th>3factlinear</th>\n",
       "      <th>4factlinear</th>\n",
       "      <th>5factlinear</th>\n",
       "      <th>3f_ffnn</th>\n",
       "      <th>4f_ffnn</th>\n",
       "      <th>5f_ffnn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.075880</td>\n",
       "      <td>-0.175525</td>\n",
       "      <td>1.360503</td>\n",
       "      <td>1.542696</td>\n",
       "      <td>1.625468</td>\n",
       "      <td>0.061885</td>\n",
       "      <td>-0.161848</td>\n",
       "      <td>0.132930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.620161</td>\n",
       "      <td>0.355801</td>\n",
       "      <td>1.478327</td>\n",
       "      <td>1.492509</td>\n",
       "      <td>1.557203</td>\n",
       "      <td>0.560735</td>\n",
       "      <td>0.541244</td>\n",
       "      <td>0.392431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.198237</td>\n",
       "      <td>-0.053156</td>\n",
       "      <td>1.610368</td>\n",
       "      <td>1.605577</td>\n",
       "      <td>1.658477</td>\n",
       "      <td>0.141161</td>\n",
       "      <td>0.118760</td>\n",
       "      <td>-0.001731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.721097</td>\n",
       "      <td>-0.824169</td>\n",
       "      <td>1.191242</td>\n",
       "      <td>1.171290</td>\n",
       "      <td>1.301063</td>\n",
       "      <td>-0.531020</td>\n",
       "      <td>-0.794841</td>\n",
       "      <td>-0.773871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.541852</td>\n",
       "      <td>0.341311</td>\n",
       "      <td>1.600140</td>\n",
       "      <td>1.591261</td>\n",
       "      <td>1.694895</td>\n",
       "      <td>0.416219</td>\n",
       "      <td>0.433857</td>\n",
       "      <td>0.328850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200</th>\n",
       "      <td>0.622349</td>\n",
       "      <td>0.910776</td>\n",
       "      <td>0.734958</td>\n",
       "      <td>0.857291</td>\n",
       "      <td>0.781614</td>\n",
       "      <td>0.677044</td>\n",
       "      <td>0.799949</td>\n",
       "      <td>0.822002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16201</th>\n",
       "      <td>0.676394</td>\n",
       "      <td>0.622349</td>\n",
       "      <td>0.694536</td>\n",
       "      <td>0.780479</td>\n",
       "      <td>0.724728</td>\n",
       "      <td>1.018917</td>\n",
       "      <td>0.743271</td>\n",
       "      <td>0.735199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16202</th>\n",
       "      <td>0.691406</td>\n",
       "      <td>0.637319</td>\n",
       "      <td>0.708815</td>\n",
       "      <td>0.794529</td>\n",
       "      <td>0.740288</td>\n",
       "      <td>1.035343</td>\n",
       "      <td>0.757536</td>\n",
       "      <td>0.748236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16203</th>\n",
       "      <td>0.691738</td>\n",
       "      <td>0.637960</td>\n",
       "      <td>0.708991</td>\n",
       "      <td>0.794650</td>\n",
       "      <td>0.740545</td>\n",
       "      <td>1.036331</td>\n",
       "      <td>0.758097</td>\n",
       "      <td>0.748671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16204</th>\n",
       "      <td>0.631615</td>\n",
       "      <td>0.578175</td>\n",
       "      <td>0.652884</td>\n",
       "      <td>0.739585</td>\n",
       "      <td>0.678862</td>\n",
       "      <td>0.966314</td>\n",
       "      <td>0.696462</td>\n",
       "      <td>0.699645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16205 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rolling_sharpe     naive  3factlinear  4factlinear  5factlinear  \\\n",
       "0            0.075880 -0.175525     1.360503     1.542696     1.625468   \n",
       "1            0.620161  0.355801     1.478327     1.492509     1.557203   \n",
       "2           -0.198237 -0.053156     1.610368     1.605577     1.658477   \n",
       "3           -0.721097 -0.824169     1.191242     1.171290     1.301063   \n",
       "4            0.541852  0.341311     1.600140     1.591261     1.694895   \n",
       "...               ...       ...          ...          ...          ...   \n",
       "16200        0.622349  0.910776     0.734958     0.857291     0.781614   \n",
       "16201        0.676394  0.622349     0.694536     0.780479     0.724728   \n",
       "16202        0.691406  0.637319     0.708815     0.794529     0.740288   \n",
       "16203        0.691738  0.637960     0.708991     0.794650     0.740545   \n",
       "16204        0.631615  0.578175     0.652884     0.739585     0.678862   \n",
       "\n",
       "        3f_ffnn   4f_ffnn   5f_ffnn  \n",
       "0      0.061885 -0.161848  0.132930  \n",
       "1      0.560735  0.541244  0.392431  \n",
       "2      0.141161  0.118760 -0.001731  \n",
       "3     -0.531020 -0.794841 -0.773871  \n",
       "4      0.416219  0.433857  0.328850  \n",
       "...         ...       ...       ...  \n",
       "16200  0.677044  0.799949  0.822002  \n",
       "16201  1.018917  0.743271  0.735199  \n",
       "16202  1.035343  0.757536  0.748236  \n",
       "16203  1.036331  0.758097  0.748671  \n",
       "16204  0.966314  0.696462  0.699645  \n",
       "\n",
       "[16205 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\"Model\": model_name, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "def evaluate_all_models(results):\n",
    "    # models = ['naive_3f', 'ffnn_3f', 'naive_4f', 'ffnn_3f', 'naive_5f', 'ffnn_5f','3factlinear', '4factlinear', '5factlinear']\n",
    "    # models = ['naive','3factlinear', '4factlinear', '5factlinear']\n",
    "    models = ['naive', '3f_ffnn', '4f_ffnn', '5f_ffnn','3factlinear', '4factlinear', '5factlinear']\n",
    "    metrics = []\n",
    "\n",
    "    for model in models:\n",
    "        if model in results.columns:\n",
    "            metrics.append(evaluate_model(results['rolling_sharpe'], results[model], model))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Model': 'naive',\n",
       "  'MAE': 0.42346372977362823,\n",
       "  'MSE': 1.8246575442019421,\n",
       "  'RMSE': 1.3507988540867002,\n",
       "  'R2': -0.05124302986756968},\n",
       " {'Model': '3f_ffnn',\n",
       "  'MAE': 1.0161727908357743,\n",
       "  'MSE': 1.8295217137888924,\n",
       "  'RMSE': 1.35259813462421,\n",
       "  'R2': -0.054045432099497415},\n",
       " {'Model': '4f_ffnn',\n",
       "  'MAE': 1.334735233223388,\n",
       "  'MSE': 3.424856054874665,\n",
       "  'RMSE': 1.850636662036788,\n",
       "  'R2': -0.9731680979958552},\n",
       " {'Model': '5f_ffnn',\n",
       "  'MAE': 0.9732048578157048,\n",
       "  'MSE': 1.7955154468097552,\n",
       "  'RMSE': 1.339968449930727,\n",
       "  'R2': -0.034453344122644},\n",
       " {'Model': '3factlinear',\n",
       "  'MAE': 0.6840248576471603,\n",
       "  'MSE': 0.8503148538027516,\n",
       "  'RMSE': 0.9221251833686962,\n",
       "  'R2': 0.5101066684576324},\n",
       " {'Model': '4factlinear',\n",
       "  'MAE': 0.6664161065771933,\n",
       "  'MSE': 0.8287739667226197,\n",
       "  'RMSE': 0.9103702360702594,\n",
       "  'R2': 0.5225170560791943},\n",
       " {'Model': '5factlinear',\n",
       "  'MAE': 0.6908348243372281,\n",
       "  'MSE': 0.8738104123369083,\n",
       "  'RMSE': 0.9347782690760993,\n",
       "  'R2': 0.49657013267295147}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_results = evaluate_all_models(test)\n",
    "metrics_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Test Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1631: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\wjlwi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1656: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mth_return</th>\n",
       "      <th>linear</th>\n",
       "      <th>mlp</th>\n",
       "      <th>ridge</th>\n",
       "      <th>lasso</th>\n",
       "      <th>elasticnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10946</th>\n",
       "      <td>0.065153</td>\n",
       "      <td>0.021697</td>\n",
       "      <td>0.028095</td>\n",
       "      <td>0.021687</td>\n",
       "      <td>0.020224</td>\n",
       "      <td>0.020162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39688</th>\n",
       "      <td>0.061436</td>\n",
       "      <td>0.025942</td>\n",
       "      <td>0.028074</td>\n",
       "      <td>0.025939</td>\n",
       "      <td>0.025101</td>\n",
       "      <td>0.025072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>0.011053</td>\n",
       "      <td>0.016035</td>\n",
       "      <td>0.014860</td>\n",
       "      <td>0.014763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47379</th>\n",
       "      <td>0.060768</td>\n",
       "      <td>0.019023</td>\n",
       "      <td>0.010773</td>\n",
       "      <td>0.019012</td>\n",
       "      <td>0.018111</td>\n",
       "      <td>0.018036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28685</th>\n",
       "      <td>0.039707</td>\n",
       "      <td>0.019021</td>\n",
       "      <td>0.012601</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.018155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47060</th>\n",
       "      <td>0.062213</td>\n",
       "      <td>0.026357</td>\n",
       "      <td>0.028813</td>\n",
       "      <td>0.026354</td>\n",
       "      <td>0.025445</td>\n",
       "      <td>0.025418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35538</th>\n",
       "      <td>0.063594</td>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.017622</td>\n",
       "      <td>0.018585</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.017319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38224</th>\n",
       "      <td>0.060625</td>\n",
       "      <td>0.027637</td>\n",
       "      <td>0.031756</td>\n",
       "      <td>0.027635</td>\n",
       "      <td>0.026746</td>\n",
       "      <td>0.026727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16851</th>\n",
       "      <td>0.013965</td>\n",
       "      <td>0.025182</td>\n",
       "      <td>0.028781</td>\n",
       "      <td>0.025177</td>\n",
       "      <td>0.024109</td>\n",
       "      <td>0.024072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45566</th>\n",
       "      <td>0.016185</td>\n",
       "      <td>0.024201</td>\n",
       "      <td>0.065111</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.022904</td>\n",
       "      <td>0.022860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mth_return    linear       mlp     ridge     lasso  elasticnet\n",
       "10946    0.065153  0.021697  0.028095  0.021687  0.020224    0.020162\n",
       "39688    0.061436  0.025942  0.028074  0.025939  0.025101    0.025072\n",
       "5363     0.016949  0.016051  0.011053  0.016035  0.014860    0.014763\n",
       "47379    0.060768  0.019023  0.010773  0.019012  0.018111    0.018036\n",
       "28685    0.039707  0.019021  0.012601  0.019010  0.018229    0.018155\n",
       "...           ...       ...       ...       ...       ...         ...\n",
       "47060    0.062213  0.026357  0.028813  0.026354  0.025445    0.025418\n",
       "35538    0.063594  0.018598  0.017622  0.018585  0.017400    0.017319\n",
       "38224    0.060625  0.027637  0.031756  0.027635  0.026746    0.026727\n",
       "16851    0.013965  0.025182  0.028781  0.025177  0.024109    0.024072\n",
       "45566    0.016185  0.024201  0.065111  0.024194  0.022904    0.022860\n",
       "\n",
       "[224 rows x 6 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "# from sklearn.metrics import r2_score\n",
    "# X, y = create_stepped_dataset(create_lagged_dataset(df_3_factor, lag=1,target_var='mth_return'),step=1,target_var='mth_return')\n",
    "# X = X.replace(np.inf, 0)\n",
    "# X = X.replace(-np.inf, 0)\n",
    "# # X\n",
    "\n",
    "# list_of_dates = df['date'].unique()\n",
    "# list_of_dates = pd.to_datetime(df['date']) # Converting to date format\n",
    "\n",
    "# percentile_70 = list_of_dates.quantile(0.7)\n",
    "# med_date = list_of_dates.loc[(list_of_dates - percentile_70).abs().idxmin()]\n",
    "# next_date = list_of_dates[list_of_dates > med_date].min()\n",
    "\n",
    "# X['date_L1'] = pd.to_datetime(X['date_L1'])\n",
    "# X_train_3f = X[X['date_L1'] < med_date]\n",
    "# y_train_3f = y[:len(X_train_3f)]\n",
    "# X_test_3f = X[X['date_L1'] == next_date]\n",
    "# y_test_3f = y[len(X_train_3f):(len(X_train_3f)+len(X_test_3f))]\n",
    "# final = y_test_3f.copy()\n",
    "# X_train_3f = X_train_3f.drop(columns='date_L1')\n",
    "# X_test_3f = X_test_3f.drop(columns='date_L1')\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# X_train_3f = scaler.fit_transform(X_train_3f)\n",
    "# X_test_3f = scaler.transform(X_test_3f)\n",
    "\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train_3f, y_train_3f)\n",
    "\n",
    "# y_pred = model.predict(X_test_3f)\n",
    "\n",
    "# model2 = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=200)\n",
    "# model2.fit(X_train_3f, y_train_3f)\n",
    "\n",
    "# y_pred2 = model2.predict(X_test_3f)\n",
    "\n",
    "# tscv = TimeSeriesSplit(n_splits = 5)\n",
    "# ridge_cv = RidgeCV(cv = tscv)\n",
    "# ridge_cv.fit(X_train_3f, y_train_3f)\n",
    "\n",
    "# model3 = Ridge(alpha = ridge_cv.alpha_)\n",
    "# model3.fit(X_train_3f, y_train_3f)\n",
    "# y_pred3 = model3.predict(X_test_3f)\n",
    "\n",
    "# lasso_cv = LassoCV(cv = tscv, random_state = 18, max_iter = 100000)\n",
    "# lasso_cv.fit(X_train_3f, y_train_3f)\n",
    "\n",
    "# # Create the Lasso model with the optimal alpha value\n",
    "# lasso_model = Lasso(alpha = lasso_cv.alpha_)\n",
    "# lasso_model.fit(X_train_3f, y_train_3f)\n",
    "# y_pred4 = lasso_model.predict(X_test_3f)\n",
    "\n",
    "# elasticnet_cv = ElasticNetCV(cv = tscv, max_iter = 100000)\n",
    "# elasticnet_cv.fit(X_train_3f, y_train_3f)\n",
    "# elasticnet_model = ElasticNet(alpha = elasticnet_cv.alpha_, l1_ratio = elasticnet_cv.l1_ratio_)\n",
    "# elasticnet_model.fit(X_train_3f, y_train_3f)\n",
    "        \n",
    "# y_pred5 = elasticnet_cv.predict(X_test_3f)\n",
    "\n",
    "# final['linear'] = y_pred\n",
    "# final['mlp'] = y_pred2\n",
    "# # final['ridge'] = y_pred3\n",
    "# # final['lasso'] = y_pred4\n",
    "# # final['elasticnet'] = y_pred5\n",
    "\n",
    "# final\n",
    "# X_test_3f\n",
    "# y_pred - y_test_3f\n",
    "# sum(abs((y_pred-y_test_3f)/y_test_3f))/len(y_test_3f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.672832028647084"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_linear = r2_score(final['mth_return'], final['linear'])\n",
    "r2_mlp = r2_score(final['mth_return'], final['mlp'])\n",
    "# r2_ridge = r2_score(final['mth_return'], final['ridge'])\n",
    "# r2_lasso = r2_score(final['mth_return'], final['lasso'])\n",
    "# r2_elasticnet = r2_score(final['mth_return'], final['elasticnet'])\n",
    "r2_mlp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
